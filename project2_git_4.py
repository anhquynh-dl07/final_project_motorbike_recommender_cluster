import streamlit as st
import pandas as pd
import numpy as np
import pickle
import joblib
import re
from underthesea import word_tokenize, pos_tag
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from scipy.sparse import csr_matrix, hstack
from datetime import datetime
from text_resources import load_teen_dict, load_stopwords
import plotly.express as px
import textwrap
from function_preprocessing_motorbike import preprocess_motobike_data
from build_model_price_anomaly_detection import detect_outliers
import os
import tempfile
import pytz
from datetime import datetime
import plotly.graph_objects as go

# ==========================================================
# 1. CACHED LOADERS
# ==========================================================

@st.cache_resource
def get_resources():
    teen_dict = load_teen_dict()
    stop_words = load_stopwords()
    return teen_dict, stop_words

teen_dict, stop_words = get_resources()


def load_models():

    vectorizer = joblib.load("tfidf_vectorizer.pkl")

    with open('tfidf_matrix.pkl', 'rb') as f:
        tfidf_matrix = pickle.load(f)

    with open("kmeans.pkl", "rb") as f:
        kmeans = pickle.load(f)

    with open("scaler.pkl", "rb") as f:
        scaler = pickle.load(f)

    ohe = joblib.load("onehot_encoder.pkl")

    imputer = joblib.load("imputer.pkl")

    with open("pca.pkl", "rb") as f:
        pca = pickle.load(f)

    return vectorizer, tfidf_matrix, kmeans, scaler, ohe, imputer, pca


@st.cache_data
def compute_clusters(df_cluster):
    # models are accessed from global scope:
    global scaler, kmeans, pca

    num_cols = ['age', 'mileage_km', 'min_price', 'max_price', 'log_price']

    X_scaled = scaler.transform(df_cluster[num_cols])
    df_cluster['cluster_label'] = kmeans.predict(X_scaled)

    pca_points = pca.transform(X_scaled)
    df_cluster['x'] = pca_points[:, 0]
    df_cluster['y'] = pca_points[:, 1]

    return df_cluster, num_cols

def load_raw_data():
    data = pd.read_excel('data_motobikes.xlsx').rename(columns={
        'Ti√™u ƒë·ªÅ': 'title',
        'ƒê·ªãa ch·ªâ': 'address',
        'M√¥ t·∫£ chi ti·∫øt': 'description',
        'Gi√°': 'price',
        'Kho·∫£ng gi√° min': 'min_price',
        'Kho·∫£ng gi√° max': 'max_price',
        'Th∆∞∆°ng hi·ªáu': 'brand',
        'D√≤ng xe': 'model',
        'NƒÉm ƒëƒÉng k√Ω': 'registration_year',
        'S·ªë Km ƒë√£ ƒëi': 'mileage_km',
        'T√¨nh tr·∫°ng': 'condition',
        'Lo·∫°i xe': 'bike_type',
        'Dung t√≠ch xe': 'engine_capacity',
        'Xu·∫•t x·ª©': 'origin',
        'Ch√≠nh s√°ch b·∫£o h√†nh': 'warranty_policy',
        'Tr·ªçng l∆∞·ª£ng': 'weight'
    })
    return data

def clean_text(text): # t·∫°o h√†m x·ª≠ l√Ω text v·ªõi text l√† chu·ªói c√°c t·ª´

    text = str(text).lower()
    text = text.replace('\n', ' ')
    text = re.sub(r'[^a-zA-Z√Ä-·ªπ0-9\s]', '', text)
    text = re.sub(r'\b\w\b', '', text)

    # Teen-code normalization
    words = text.split()
    words = [teen_dict.get(w, w) for w in words]
    text = ' '.join(words)

    # Tokenize & POS filter
    tokenized = word_tokenize(text)
    pos_tagged_text = pos_tag(" ".join(tokenized))
    filtered_words = [word for word, tag in pos_tagged_text if tag != 'T']

    # Stopword removal
    clean_words = [word for word in filtered_words if word not in stop_words]

    # Return string (not list), same as df['content_clean_cosine']
    return " ".join(clean_words)

def clean_df_for_recommender(df):
    ### For numeric part of vector

    # clean price
    df['price'] = (
    df['price']
    .astype(str)
    .str.replace('[^0-9]', '', regex=True)   # ch·ªâ gi·ªØ l·∫°i ch·ªØ s·ªë
    .replace('', np.nan)
    .astype(float)
)
    def parse_minmax_price(s):
        if pd.isna(s):
            return np.nan
        s = str(s).lower().replace("tr", "").replace(" ", "")
        try:
            return float(s) * 1_000_000
        except:
            return np.nan

    df['min_price'] = df['min_price'].apply(parse_minmax_price)
    df['max_price'] = df['max_price'].apply(parse_minmax_price)

    # X√°c ƒë·ªãnh num/ non-num cols ƒë·ªÉ fill NA
    num_cols = df.select_dtypes(include=["int64", "float64"]).columns
    cat_cols = df.select_dtypes(include=["object"]).columns

    # Fill NA (num -> median, non-num -> mode)
    # 1. Numeric imputation
    num_imputer = joblib.load('imputer.pkl')
    df[num_cols] = num_imputer.fit_transform(df[num_cols])

    # 2. Categorical imputation
    cat_imputer = SimpleImputer(strategy="most_frequent")
    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

    # Thay th·∫ø c√°c gi√° tr·ªã kh√¥ng r√µ trong c·ªôt 'engine_capacity'
    df['engine_capacity'] = df['engine_capacity'].replace(
        ['Kh√¥ng bi·∫øt r√µ', 'ƒêang c·∫≠p nh·∫≠t', 'Nh·∫≠t B·∫£n'],
        'Unknown'
    )

    # Thay th·∫ø c√°c gi√° tr·ªã kh√¥ng r√µ trong c·ªôt 'origin', gi·ªØ nguy√™n nh√≥m "B·∫£o h√†nh h√£ng" ƒë·ªÉ x·ª≠ l√Ω text
    df['origin'] = df['origin'].replace(
        ['ƒêang c·∫≠p nh·∫≠t', 'N∆∞·ªõc kh√°c'],
        'N∆∞·ªõc kh√°c'
    )

    # Chu·∫©n h√≥a registration_year
    df['registration_year'] = (
        df['registration_year']
        .astype(str)
        .str.lower()
        .str.replace('tr∆∞·ªõc nƒÉm', '1980', regex=False)
        .str.extract('(\d{4})')[0]
    )
    # Chuy·ªÉn sang numeric, nh·ªØng gi√° tr·ªã kh√¥ng chuy·ªÉn ƒë∆∞·ª£c s·∫Ω th√†nh NA
    df['registration_year'] = pd.to_numeric(df['registration_year'], errors='coerce')

    # Fill NA ban ƒë·∫ßu
    df['registration_year'] = df['registration_year'].fillna(df['registration_year'].median())

    # G·∫Øn gi√° tr·ªã b·∫•t h·ª£p l·ªá th√†nh NA
    df.loc[
        (df['registration_year'] < 1980) | (df['registration_year'] > 2025),
        'registration_year'
    ] = np.nan

    # Fill NA sau khi lo·∫°i b·∫•t h·ª£p l·ªá
    df['registration_year'] = df['registration_year'].fillna(df['registration_year'].median())

    # Th√™m bi·∫øn age
    current_year = datetime.now().year
    df['age'] = current_year - df['registration_year']

    # gom nh√≥m brand hi·∫øm v√† t·∫°o c·ªôt 'segment'
    brand_counts = df['brand'].value_counts()
    rare_brands = brand_counts[brand_counts < 50].index
    df['brand_grouped'] = df['brand'].replace(rare_brands, 'H√£ng kh√°c')

    def group_model(x):
        counts = x.value_counts()
        rare_models = counts[counts < 100].index
        return x.replace(rare_models, 'D√≤ng kh√°c')

    df['model_grouped'] = df.groupby('brand_grouped')['model'].transform(group_model)
    df['segment'] = df['brand_grouped'] + '_' + df['model_grouped']

    # One hot encoding 'bike_type', 'engine_capacity'
    encoded = ohe.transform(df[['bike_type', 'engine_capacity']])
    encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(['bike_type', 'engine_capacity']))
    # merge back to original dataframe
    df = pd.concat([df, encoded_df], axis=1)

    # numeric features
    num_features = ['price','mileage_km','min_price','max_price','age', 'registration_year']
    # log normalize numeric features
    normalized_features = []
    for col in num_features:
        new_col = col + "_log"
        df[new_col] = np.log1p(df[col])
        normalized_features.append(new_col)

    # t·∫°o feature brand_meanprice
    brand_mean_log = df.groupby('brand')['price_log'].mean().rename('brand_meanprice')
    df = df.merge(brand_mean_log, on='brand', how='left')
    normalized_features.append('brand_meanprice')

    # features to turn to a vector: 
    onehot_features = ohe.get_feature_names_out(['bike_type', 'engine_capacity']).tolist()
    num_features = onehot_features + normalized_features

    # X·ª≠ l√Ω NaN (n·∫øu c√≥) ƒë·ªÉ t·∫°o dense vector cho vi·ªác t√≠nh to√°n cosine similarity l√∫c sau
    X_num = df[num_features].copy()

    # 1Ô∏è‚É£ Impute missing values
    # imputer = SimpleImputer(strategy="median")
    X_num_imputed = imputer.fit_transform(X_num)

    # 2Ô∏è‚É£ Scaling for num features
    scaler = StandardScaler()
    X_num_scaled = scaler.fit_transform(X_num_imputed)

    ### For text part of vector
    # ·ªû ƒë√¢y ƒë√£ load tfidf_matrix n√™n kh√¥ng x·ª≠ l√Ω ph·∫ßn text n·ªØa

    ### T·∫°o vector ƒë·∫ßu v√†o b·∫±ng c√°ch k·∫øt h·ª£p vector TF-IDF v√† array num col (X_num_scaled)
    # from scipy.sparse import csr_matrix, hstack
    # Chuy·ªÉn array X_num_scaled th√†nh matrix d·∫°ng sparse (ko store c√°c gi√° tr·ªã 0)
    X_num_sparse = csr_matrix(X_num_scaled)

    # Gh√©p ma tr·∫≠n TF-IDF v√† ma tr·∫≠n X_num_sparse theo chi·ªÅu ngang
    X_final = hstack([tfidf_matrix, X_num_sparse])

    return df, X_final

def clean_df_for_clustering(df_cluster):
    cols_drop = ['title', 'address', 'description', 'Href']
    df_cluster = df_cluster.drop(columns=[c for c in cols_drop if c in df_cluster.columns], errors='ignore')
    df_cluster = df_cluster.drop(columns=['warranty_policy', 'weight', 'condition'], errors='ignore')
    df_cluster = df_cluster.dropna()

    # Clean price
    df_cluster['price'] = (
        df_cluster['price'].astype(str)
        .str.replace('[^0-9]', '', regex=True)
        .replace('', np.nan).astype(float)
    )

    # Minimal cleaning df price for display
    if 'price' in df_cluster.columns:
        df_cluster['price'] = df_cluster['price'].astype(str).str.replace('[^0-9]', '', regex=True)
        df_cluster.loc[df_cluster['price'] == '', 'price'] = np.nan
        df_cluster['price'] = pd.to_numeric(df_cluster['price'], errors='coerce')

    # ensure registration_year numeric
    if 'registration_year' in df_cluster.columns:
        df_cluster['registration_year'] = (
            df_cluster['registration_year'].astype(str)
            .str.lower()
            .str.replace('tr∆∞·ªõc nƒÉm', '1980', regex=False)
            .str.extract(r'(\d{4})')[0]
        )
        df_cluster['registration_year'] = pd.to_numeric(df_cluster['registration_year'], errors='coerce')
        df_cluster.loc[(df_cluster['registration_year'] < 1980) | (df_cluster['registration_year'] > 2025), 'registration_year'] = np.nan
    
    def parse_price(s):
        if pd.isna(s): return np.nan
        s = str(s).lower().replace("tr", "").replace(" ", "")
        try: return float(s) * 1_000_000
        except: return np.nan

    df_cluster['min_price'] = df_cluster['min_price'].apply(parse_price)
    df_cluster['max_price'] = df_cluster['max_price'].apply(parse_price)

    df_cluster = df_cluster[~(df_cluster['price'] == 0)]

    # Remove invalid engine_capacity
    df_cluster = df_cluster[~df_cluster['engine_capacity'].astype(str).str.contains("Nh·∫≠t B·∫£n", na=False)]

    # Clean origin
    df_cluster = df_cluster[~df_cluster['origin'].astype(str).str.contains('B·∫£o h√†nh h√£ng', case=False, na=False)]
    df_cluster['origin'] = df_cluster['origin'].replace(['ƒêang c·∫≠p nh·∫≠t', 'N∆∞·ªõc kh√°c'], 'N∆∞·ªõc kh√°c')

    # Registration year
    df_cluster['registration_year'] = (
        df_cluster['registration_year'].astype(str)
        .str.lower()
        .str.replace('tr∆∞·ªõc nƒÉm', '1980')
        .str.extract('(\d{4})')[0]
    ).astype(float)

    df_cluster.loc[(df_cluster['registration_year'] < 1980) | (df_cluster['registration_year'] > 2025),
            'registration_year'] = np.nan

    df_cluster["age"] = 2025 - df_cluster["registration_year"]

    # Log transforms
    numeric_cols = ['age', 'mileage_km', 'min_price', 'max_price', 'price']
    for c in numeric_cols:
        df_cluster[f"log_{c}"] = np.log1p(df_cluster[c])

    df_cluster = df_cluster.dropna(subset=numeric_cols)

    return df_cluster

# ==========================================================
# LOAD EVERYTHING (CACHED)
# ==========================================================

@st.cache_data
def get_clean_recommender_data():
    df_raw = load_raw_data()
    return clean_df_for_recommender(df_raw.copy())

@st.cache_data
def get_cluster_data():
    df_raw = load_raw_data()
    df_cluster = clean_df_for_clustering(df_raw.copy())
    df_cluster, num_cols = compute_clusters(df_cluster)
    return df_cluster, num_cols

# Load models (already cached)
vectorizer, tfidf_matrix, kmeans, scaler, ohe, imputer, pca = load_models()

# Load cleaned datasets
df_clean, X_final = get_clean_recommender_data()
df_cluster, num_cols = get_cluster_data()


# ==========================================================
# FUNCTIONS
# ==========================================================
def preprocess_user_input(price, min_price, max_price, mileage_km, registration_year):
    age = 2025 - registration_year
    log_price = np.log1p(price)
    X = np.array([[age, mileage_km, min_price, max_price, log_price]])
    return scaler.transform(X)

def get_top_n_similar_by_content(df, X_final, title, top_n=5):
    """
    Given a bike title, return top N most similar bikes based on
    combined TF-IDF + numeric features vector.

    Params:
        df (DataFrame): cleaned dataframe returned from clean_df_for_recommender
        X_final (sparse matrix): combined feature matrix
        title (str): the selected bike title
        top_n (int): number of similar bikes to return

    Returns:
        df_recommend (DataFrame): rows of top-N similar bikes
        scores (list): similarity scores
    """

    # Find the index of the selected bike
    matches = df.index[df['title'] == title]

    if len(matches) == 0:
        return None, []   # title not found

    idx = matches[0]

    # Compute cosine similarity for this single item
    sims = cosine_similarity(X_final[idx], X_final).flatten()

    # Sort by similarity (descending), ignore itself
    ranked_indices = np.argsort(sims)[::-1]

    # Remove itself
    ranked_indices = ranked_indices[ranked_indices != idx]

    # Take top-N
    top_indices = ranked_indices[:top_n]
    top_scores = sims[top_indices]

    # Return matching rows + scores
    df_recommend = df.iloc[top_indices].copy()
    df_recommend['similarity_score'] = top_scores

    return df_recommend, top_scores.tolist()

# helper: safe format number
def fmt_vnd(x):
    try:
        return f"{int(x):,} VNƒê"
    except:
        return '-'

MODEL_PATH = "motobike_price_prediction_model.pkl"
TRAINING_DATA = "data_motobikes.xlsx"  # optional, used to compute brand_meanprice & grouping to match train

@st.cache_resource
def load_model(path=MODEL_PATH):
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return pickle.load(f)

@st.cache_data
def build_training_helpers(path=TRAINING_DATA):
    """
    Load training data & build grouping rules + statistical thresholds
    (p10/p90, residual mean/std) for anomaly detection.
    """
    if not os.path.exists(path):
        return None

    try:
        df_train = preprocess_motobike_data(path)
        # =============== LOAD MODELS ===============
        with open("unsup_scaler.pkl", "rb") as f:
            scaler_nom = pickle.load(f)

        with open("scaler.pkl", "rb") as f:
            scaler = pickle.load(f)

        with open("kmeans_model.pkl", "rb") as f:
            kmeans_anom = pickle.load(f)

        with open("kmeans.pkl", "rb") as f:
            kmeans = pickle.load(f)

        # =============== 1) BRAND GROUPING ==================
        brand_counts = df_train['brand'].value_counts()
        rare_brands = set(brand_counts[brand_counts < 50].index)

        # model grouping by brand_grouped
        model_group_maps = {}
        for bg, g in df_train.groupby('brand_grouped'):
            counts = g['model'].value_counts()
            rare_models = set(counts[counts < 100].index)
            model_group_maps[bg] = rare_models

        # mean price for brand
        brand_mean_map = df_train.groupby('brand')['brand_meanprice'].first().to_dict()

        # =============== 2) PRICE P10/P90 BY SEGMENT ==================
        seg_price_stats = (
            df_train.groupby('segment')['price']
                    .quantile([0.10, 0.90])
                    .unstack(level=1)
                    .rename(columns={0.10:'p10', 0.90:'p90'})
        ).reset_index()

        seg_price_map = seg_price_stats.set_index('segment').to_dict('index')
        # format: seg_price_map[segment] = {'p10':..., 'p90':...}

        # =============== 3) RESIDUAL STATS BY SEGMENT ==================

        # Load model
        with open(MODEL_PATH, 'rb') as f:
            model = pickle.load(f)

        # Define cols
        cat_cols = ['segment','bike_type','origin','engine_capacity']
        num_cols = ['age','mileage_km','min_price','max_price','brand_meanprice']

        # Build matrix
        X = df_train[cat_cols + num_cols]
        # y = df['log_price']

        # Predict price
        df_train['price_hat'] = np.expm1(model.predict(X))
        df_train['resid'] = df_train['price'] - df_train['price_hat']  # price_hat t·ª´ preprocess

        seg_resid_stats = (
            df_train.groupby('segment')['resid']
                    .agg(['mean', 'std'])
                    .rename(columns={'mean': 'resid_mean', 'std': 'resid_std'})
        ).reset_index()

        seg_resid_map = seg_resid_stats.set_index('segment').to_dict('index')
        # format: seg_resid_map[seg] = {'resid_mean':..., 'resid_std':...}

        
        num_cols = ['age','mileage_km','min_price','max_price','log_price']

        X = df_train[num_cols].dropna()
        X_scaled = scaler.transform(X)

        df_train['cluster_label'] = kmeans.predict(X_scaled)

        cluster_summary = (
            df_train.groupby('cluster_label')
                    .agg(
                        avg_price=('price','mean'),
                        avg_age=('age','mean'),
                        avg_mileage=('mileage_km','mean'),
                        count=('cluster_label','size')
                    )
                    .to_dict('index')
        )

        return {
            'rare_brands': rare_brands,
            'model_group_maps': model_group_maps,
            'brand_mean_map': brand_mean_map,
            'seg_price_map': seg_price_map,
            'seg_resid_map': seg_resid_map,
            'cluster_summary': cluster_summary,
            'cluster_model': kmeans,
            'cluster_scaler': scaler
                }

    except Exception as e:
        print("Error building helpers:", e)
        return None


helpers = build_training_helpers(TRAINING_DATA)
model = load_model(MODEL_PATH)



# ==========================================================
# STREAMLIT PAGES
# ==========================================================
st.set_page_config(
    page_title="H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y",
    page_icon="üèçÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y")
st.image("xe_may_cu2.jpg",  width=1500)

st.sidebar.markdown("""
## H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y
""")

st.sidebar.markdown("""
### Th√†nh vi√™n nh√≥m 6
1. V≈© Th·ªã Ng·ªçc Anh
2. Nguy·ªÖn Ph·∫°m Qu·ª≥nh Anh
""")

st.sidebar.markdown("### Menu")   
menu = ["Gi·ªõi thi·ªáu", "B√†i to√°n nghi·ªáp v·ª•", "ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o",
        "G·ª£i √Ω m·∫´u xe t∆∞∆°ng t·ª±", "X√°c ƒë·ªãnh ph√¢n kh√∫c xe m√°y"]

# page = st.sidebar.selectbox("Menu", menu, label_visibility="collapsed")
page = st.sidebar.selectbox(
    "Menu",
    menu,
    label_visibility="collapsed",
    key="menu_select",
    # enable full width
)


# ==========================================================
# STYLES
# ==========================================================

BASE_CSS = """
<style>
:root{
  --accent-1: #ffde37;       /* Your yellow */
  --accent-2: #e5c620;       /* Slightly darker yellow for gradients */
  --muted: #4a4a4a;
  --card-bg: #fff7c2;        /* Soft light yellow background */
  --glass: rgba(255,255,255,0.55);
}

/* Background */
html, body {
  background: linear-gradient(180deg, #fff5a0 0%, #ffef73 100%);
  color: #000000 !important;
}

/* Header / hero section */
.header-hero {
  background: linear-gradient(90deg, var(--accent-1), var(--accent-2));
  padding: 22px;
  border-radius: 12px;
  color: #000000;
  font-weight: 600;
  margin-bottom: 18px;
  box-shadow: 0 6px 24px rgba(0,0,0,0.12);
}

/* Small muted text */
.small-muted {
  color: var(--muted);
  font-size: 13px;
}

/* Cards */
.card {
  background: var(--card-bg);
  padding: 14px;
  border-radius: 12px;
  box-shadow: 0 6px 18px rgba(0,0,0,0.10);
  color: #000000;
}

/* Base typography */
h1, h2, h3, h4, h5, h6, p, span, div {
  color: #000000 !important;
}

/* Bike title / subtitles */
.bike-title{
  font-size:18px;
  font-weight:700;
  margin-bottom:4px;
}

.bike-sub{
  font-size:13px;
  color:var(--muted);
  margin-bottom:6px;
}

/* Cluster cards */
.cluster-card{
  padding:18px;
  border-radius:12px;
  color:#000000;
  margin-bottom:12px;
  font-weight:600;
}

/* Cluster variants using your yellow palette */
.cluster-0{
  background:linear-gradient(135deg, #ffeb7a, #ffde37);
}
.cluster-1{
  background:linear-gradient(135deg, #ffe45c, #e5c620);
}
.cluster-2{
  background:linear-gradient(135deg, #fff1a1, #ffde37);
}
</style>
"""

st.markdown(BASE_CSS, unsafe_allow_html=True)

@st.cache_data
def load_reference_data():
    return preprocess_motobike_data(TRAINING_DATA)

df_ref = load_reference_data()
brand_list = sorted(df_ref['brand_grouped'].dropna().unique())
model_list = sorted(df_ref['model_grouped'].dropna().unique())
bike_type_list = sorted(df_ref['bike_type'].dropna().unique())
origin_list = sorted(df_ref['origin'].dropna().unique())
engine_capacity_list = sorted(df_ref['engine_capacity'].dropna().unique())

# ==========================================================
# PAGE CONTENT
# ==========================================================

if page == 'Gi·ªõi thi·ªáu':
    # st.title("H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y")
    st.markdown("""
        <h1 style='font-size:35px; font-weight:800; margin-bottom:8px;'>
            Gi·ªõi thi·ªáu
        </h1>
        <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
    """, unsafe_allow_html=True)    
    # st.image("xe_may_cu2.jpg")
    st.subheader("[Trang ch·ªß Ch·ª£ T·ªët](https://www.chotot.com/)")

        # Function for light yellow pad header
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)
    
    yellow_pad_header('Gi·ªõi thi·ªáu d·ª± √°n')
    st.markdown('''ƒê√¢y l√† d·ª± √°n x√¢y d·ª±ng h·ªá th·ªëng h·ªó tr·ª£ **g·ª£i √Ω m·∫´u xe m√°y t∆∞∆°ng t·ª±** 
v√† **ph√¢n kh√∫c xe m√°y b·∫±ng ph∆∞∆°ng ph√°p ph√¢n c·ª•m** tr√™n n·ªÅn t·∫£ng *Ch·ª£ T·ªët* ‚Äì 
trong kh√≥a ƒë·ªì √°n t·ªët nghi·ªáp Data Science and Machine Learning 2024 l·ªõp DL07_K308 c·ªßa nh√≥m 6.

Th√†nh vi√™n nh√≥m g·ªìm c√≥:
1. V≈© Th·ªã Ng·ªçc Anh  
2. Nguy·ªÖn Ph·∫°m Qu·ª≥nh Anh
''')

    yellow_pad_header('M·ª•c ti√™u c·ªßa d·ª± √°n')
    st.markdown("""
    **1. X√¢y d·ª±ng m√¥ h√¨nh ƒë·ªÅ xu·∫•t th√¥ng minh:**
    - ƒê·ªÅ xu·∫•t c√°c m·∫´u xe m√°y t∆∞∆°ng ƒë·ªìng cho m·ªôt m·∫´u ƒë∆∞·ª£c ch·ªçn ho·∫∑c theo t·ª´ kh√≥a t√¨m ki·∫øm.
    - K·∫øt h·ª£p nhi·ªÅu ngu·ªìn th√¥ng tin (th√¥ng s·ªë k·ªπ thu·∫≠t, h√¨nh ·∫£nh, m√¥ t·∫£, gi√°, ƒë√°nh gi√°) ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c.

    **2. Ph√¢n kh√∫c th·ªã tr∆∞·ªùng xe m√°y:**
    - Ph√¢n lo·∫°i s·∫£n ph·∫©m theo nh√≥m theo t·ªáp gi√°, tu·ªïi xe, kho·∫£ng gi√° t·ªëi thi·ªÉu/t·ªëi ƒëa.
    - H·ªó tr·ª£ ƒë·ªãnh gi√° v√† x√¢y d·ª±ng chi·∫øn l∆∞·ª£c marketing hi·ªáu qu·∫£ h∆°n.
    """)

    yellow_pad_header('Ph√¢n c√¥ng c√¥ng vi·ªác')
    st.write("""
    - **X·ª≠ l√Ω d·ªØ li·ªáu:** Ng·ªçc Anh v√† Qu·ª≥nh Anh  
    - **G·ª£i √Ω xe m√°y b·∫±ng Gensim:** Qu·ª≥nh Anh  
    - **G·ª£i √Ω xe m√°y b·∫±ng Cosine similarity:** Qu·ª≥nh Anh v√† Ng·ªçc Anh 
    - **Ph√¢n kh√∫c xe m√°y b·∫±ng ph∆∞∆°ng ph√°p ph√¢n c·ª•m:** Ng·ªçc Anh  
    - **L√†m slide:** Ng·ªçc Anh v√† Qu·ª≥nh Anh  
    - **Giao di·ªán Streamlit:** Qu·ª≥nh Anh v√† Ng·ªçc Anh
    """)
    
elif page == 'B√†i to√°n nghi·ªáp v·ª•':
    st.markdown("""
    <h1 style='font-size:35px; font-weight:800; margin-bottom:8px;'>
        B√†i to√°n nghi·ªáp v·ª•
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
""", unsafe_allow_html=True)
    # Function for light yellow pad header
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)

    yellow_pad_header('V·∫•n ƒë·ªÅ nghi·ªáp v·ª•')
    st.markdown("""
        - Ng∆∞·ªùi d√πng g·∫∑p kh√≥ khƒÉn khi t√¨m xe ph√π h·ª£p trong h√†ng trƒÉm l·ª±a ch·ªçn.
        - Ch∆∞a c√≥ h·ªá th·ªëng g·ª£i √Ω xe t∆∞∆°ng t·ª± khi ng∆∞·ªùi d√πng ch·ªçn m·ªôt m·∫´u c·ª• th·ªÉ ho·∫∑c t√¨m ki·∫øm theo t·ª´ kh√≥a.
        - Th·ªã tr∆∞·ªùng xe m√°y r·∫•t ƒëa d·∫°ng ‚Üí kh√≥ nh·∫≠n di·ªán c√°c ph√¢n kh√∫c r√µ r√†ng.
        - C·∫ßn h·ªá th·ªëng g·ª£i √Ω & ph√¢n kh√∫c t·ª± ƒë·ªông ƒë·ªÉ h·ªó tr·ª£ ng∆∞·ªùi d√πng v√† ƒë·ªôi ng≈© ph√¢n t√≠ch.""")

    yellow_pad_header('B√†i to√°n ƒë·∫∑t ra')
    st.markdown("""
        1. X√¢y d·ª±ng m√¥ h√¨nh **G·ª£i √Ω xe t∆∞∆°ng t·ª±**
        - S·ª≠ d·ª•ng c√°c ƒë·∫∑c tr∆∞ng t·ª´ m√¥ t·∫£ xe v√† th√¥ng s·ªë k·ªπ thu·∫≠t
        - G·ª£i √Ω c√°c m·∫´u xe t∆∞∆°ng t·ª± v·ªõi xe ƒë∆∞·ª£c ch·ªçn ho·∫∑c theo t·ª´ kh√≥a t√¨m ki·∫øm.
        &nbsp;
        2. X√¢y d·ª±ng m√¥ h√¨nh **Ph√¢n kh√∫c th·ªã tr∆∞·ªùng xe b·∫±ng ph∆∞∆°ng ph√°p ph√¢n c·ª•m**
        - Ph√¢n c·ª•m th·ªã tr∆∞·ªùng xe m√°y d·ª±a c√°c ƒë·∫∑c tr∆∞ng gi√° xe, tu·ªïi xe, s·ªë km ƒë√£ ch·∫°y, kho·∫£ng gi√° t·ªëi thi·ªÉu, t·ªëi ƒëa.
        - Gi√∫p nh·∫≠n di·ªán v√† ph√¢n lo·∫°i xe theo c√°c ph√¢n kh√∫c kh√°c nhau.
                """)
    
    yellow_pad_header('Ph·∫°m vi tri·ªÉn khai')
    st.markdown("""
        **1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† chu·∫©n h√≥a**:  
            - Chu·∫©n h√≥a c√°c th√¥ng s·ªë c·ªßa xe.  
            - L√†m s·∫°ch d·ªØ li·ªáu v√† chu·∫©n h√≥a tr∆∞·ªùng th√¥ng tin cho m√¥ h√¨nh.  
                
        **2. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v√† t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng**:  
            - S·ª≠ d·ª•ng **TF-IDF Vectorizer** ƒë·ªÉ m√£ h√≥a m√¥ t·∫£ v√† th√¥ng tin k·ªπ thu·∫≠t.  
            - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng b·∫±ng **gensim similarity** v√† **cosine similarity**.  
            - Ch·ªçn ph∆∞∆°ng ph√°p cho **ƒëi·ªÉm cao h∆°n** v√† **nghƒ©a ƒë√∫ng h∆°n** ƒë·ªÉ ƒë∆∞a v√†o h·ªá th·ªëng g·ª£i √Ω.  
                
        **3. Ph√¢n c·ª•m th·ªã tr∆∞·ªùng (Clustering)**:  
            - Th·ª≠ nghi·ªám tr√™n c√°c thu·∫≠t to√°n: KMeans, Bisecting KMeans, Agglomerative Clustering  
            - ƒê√°nh gi√° b·∫±ng inertia, silhouette score, t√≠nh di·ªÖn gi·∫£i.  
            - Ch·ªçn **KMeans** v√¨ c√≥ hi·ªáu su·∫•t ·ªïn ƒë·ªãnh, d·ªÖ di·ªÖn gi·∫£i v√† ranh gi·ªõi c·ª•m ph√π h·ª£p h∆°n v·ªõi d·ªØ li·ªáu.

        **4. X√¢y d·ª±ng GUI tr√™n Streamlit**:  
            - Cho ph√©p ng∆∞·ªùi d√πng **ch·ªçn xe trong danh s√°ch** ho·∫∑c **nh·∫≠p m√¥ t·∫£ xe** ‚Üí tr·∫£ v·ªÅ **danh s√°ch m·∫´u xe t∆∞∆°ng t·ª± c√≥ trong s√†n**.  
            - Cho ph√©p **nh·∫≠p t√™n xe** ‚Üí hi·ªÉn th·ªã **xe thu·ªôc c·ª•m/ph√¢n kh√∫c n√†o**.
                """)

    yellow_pad_header('Thu th·∫≠p d·ªØ li·ªáu')
    st.markdown("""
        - B·ªô d·ªØ li·ªáu g·ªìm **7.208 tin ƒëƒÉng** v·ªõi **18 thu·ªôc t√≠nh** (th∆∞∆°ng hi·ªáu, d√≤ng xe, s·ªë km, nƒÉm ƒëƒÉng k√Ω, gi√° ni√™m y·∫øt, m√¥ t·∫£, v.v‚Ä¶) ƒë∆∞·ª£c thu th·∫≠p t·ª´ n·ªÅn t·∫£ng **Ch·ª£ T·ªët** (tr∆∞·ªõc ng√†y 01/07/2025).
        - B·ªô d·ªØ li·ªáu bao g·ªìm c√°c th√¥ng tin sau:
            - **id**: s·ªë th·ª© t·ª± c·ªßa s·∫£n ph·∫©m trong b·ªô d·ªØ li·ªáu  
            - **Ti√™u ƒë·ªÅ**: t·ª±a ƒë·ªÅ b√†i ƒëƒÉng b√°n s·∫£n ph·∫©m  
            - **Gi√°**: gi√° b√°n c·ªßa xe m√°y  
            - **Kho·∫£ng gi√° min**: gi√° s√†n ∆∞·ªõc t√≠nh c·ªßa xe m√°y  
            - **Kho·∫£ng gi√° max**: gi√° tr·∫ßn ∆∞·ªõc t√≠nh c·ªßa xe m√°y  
            - **ƒê·ªãa ch·ªâ**: ƒë·ªãa ch·ªâ giao d·ªãch (ph∆∞·ªùng, qu·∫≠n, th√†nh ph·ªë H·ªì Ch√≠ Minh)  
            - **M√¥ t·∫£ chi ti·∫øt**: m√¥ t·∫£ th√™m v·ªÅ s·∫£n ph·∫©m ‚Äî ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t, t√¨nh tr·∫°ng, th√¥ng tin kh√°c  
            - **Th∆∞∆°ng hi·ªáu**: h√£ng s·∫£n xu·∫•t (Honda, Yamaha, Piaggio, SYM‚Ä¶)  
            - **D√≤ng xe**: d√≤ng xe c·ª• th·ªÉ (Air Blade, Vespa, Exciter, LEAD, Vario, ‚Ä¶)  
            - **NƒÉm ƒëƒÉng k√Ω**: nƒÉm ƒëƒÉng k√Ω l·∫ßn ƒë·∫ßu c·ªßa xe  
            - **S·ªë km ƒë√£ ƒëi**: s·ªë kilomet xe ƒë√£ v·∫≠n h√†nh  
            - **T√¨nh tr·∫°ng**: t√¨nh tr·∫°ng hi·ªán t·∫°i (v√≠ d·ª•: ƒë√£ s·ª≠ d·ª•ng)  
            - **Lo·∫°i xe**: Xe s·ªë, Tay ga, Tay c√¥n/Moto  
            - **Dung t√≠ch xe**: dung t√≠ch xi-lanh (v√≠ d·ª•: D∆∞·ªõi 50cc, 50‚Äì100cc, 100‚Äì175cc, ‚Ä¶)  
            - **Xu·∫•t x·ª©**: qu·ªëc gia s·∫£n xu·∫•t (Vi·ªát Nam, ƒê√†i Loan, Nh·∫≠t B·∫£n, ...)  
            - **Ch√≠nh s√°ch b·∫£o h√†nh**: th√¥ng tin b·∫£o h√†nh n·∫øu c√≥  
            - **Tr·ªçng l∆∞·ª£ng**: tr·ªçng l∆∞·ª£ng ∆∞·ªõc t√≠nh c·ªßa xe  
            - **Href**: ƒë∆∞·ªùng d·∫´n t·ªõi b√†i ƒëƒÉng s·∫£n ph·∫©m 
                """)

elif page == 'ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o':
    st.markdown("""
    <h1 style='font-size:35px; font-weight:800; margin-bottom:8px;'>
        ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
""", unsafe_allow_html=True)
    
    # Function for light yellow pad header
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True) 

    yellow_pad_header('Th·ªëng k√™ m√¥ t·∫£ s∆° b·ªô')


    st.markdown("""        
    B·ªô d·ªØ li·ªáu g·ªìm **7.208 tin ƒëƒÉng** v·ªõi **18 thu·ªôc t√≠nh** (th∆∞∆°ng hi·ªáu, d√≤ng xe, s·ªë km, nƒÉm ƒëƒÉng k√Ω, gi√° ni√™m y·∫øt, m√¥ t·∫£‚Ä¶) ƒë∆∞·ª£c thu th·∫≠p t·ª´ n·ªÅn t·∫£ng **Ch·ª£ T·ªët** (tr∆∞·ªõc ng√†y 01/07/2025).  
                """)
    
    image_width = 600
    # Hi·ªÉn th·ªã 4 bi·ªÉu ƒë·ªì d·∫°ng l∆∞·ªõi 2x2
    col1, col2 = st.columns(2)
    with col1:
        st.image("brand_grouped_count.png", width=image_width) # Th√™m width=500
        st.image("age_bin_stats.png", width=image_width)       # Th√™m width=500

    with col2:
        st.image("price_bin_stats.png", width=image_width)     # Th√™m width=500
        st.image("mileage_bin_stats.png", width=image_width)   # Th√™m width=500

    yellow_pad_header('M√¥ h√¨nh g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª±')

    # with open("data/data_motobikes.xlsx", "rb") as f:
    #     st.download_button(
    #         label="üì• T·∫£i xu·ªëng d·ªØ li·ªáu xe m√°y (Excel)",
    #         data=f,
    #         file_name="data_motobikes.xlsx",
    #         mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    #     )

    st.markdown('#### 1. H∆∞·ªõng x·ª≠ l√Ω')
    st.write('''
             - Chu·∫©n h√≥a v√† l√†m s·∫°ch d·ªØ li·ªáu.
             - Chia kho·∫£ng m·ªôt s·ªë ƒë·∫∑c tr∆∞ng ki·ªÉu s·ªë ƒë·ªÉ t·∫°o th√™m c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i m·ªõi (kho·∫£ng gi√°, t√¨nh tr·∫°ng d·ª±a theo s·ªë km ch·∫°y, tu·ªïi xe, dung t√≠ch xe)
             - Gom c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i th√†nh bi·∫øn text
             - L√†m s·∫°ch text v√† tokenize, x√¢y d·ª±ng ma tr·∫≠n t∆∞∆°ng ƒë·ªìng (sparse matrix) gi·ªØa c√°c vƒÉn b·∫£n ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô gi·ªëng nhau
             - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng b·∫±ng gensim v√† cosine similarity
                 - Tr∆∞·ªùng h·ª£p 1: g·ª£i √Ω xe theo id s·∫£n ph·∫©m ƒë∆∞·ª£c ch·ªçn
                    - Ng∆∞·ªùi d√πng ch·ªçn xe t·ª´ danh s√°ch xe trong t·∫≠p d·ªØ li·ªáu
                    - D·ª±a tr√™n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng, t√¨m c√°c xe c√≥ similarity score cao nh·∫•t.
                    - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng trung b√¨nh gi·ªØa 5 m·∫´u g·ª£i √Ω cho m·ªôt m·∫´u, sau ƒë√≥ √°p d·ª•ng cho 7000 m·∫´u trong t·∫≠p d·ªØ li·ªáu v√† t√≠nh trung b√¨nh.

                 - Tr∆∞·ªùng h·ª£p 2: g·ª£i √Ω xe theo c·ª•m t·ª´ kh√≥a t√¨m ki·ªÉm (vd: ‚Äúhonda vision xanh d∆∞·ªõi 15 tri·ªáu‚Äù)
                    - Ng∆∞·ªùi d√πng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm. 
                    - X·ª≠ l√Ω t·ª´ kh√≥a v√† chuy·ªÉn t·ª´ kh√≥a th√†nh vector s·ªë d·ª±a tr√™n t·ª´ ƒëi·ªÉn v√† TF-IDF
                    - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa t·ª´ kh√≥a v√† t·∫•t c·∫£ xe trong d·ªØ li·ªáu. 
                    - S·∫Øp x·∫øp v√† l·∫•y ra 5 xe g·ª£i √Ω ph√π h·ª£p nh·∫•t.
                    - Cho danh s√°ch 10 c·ª•m t·ª´ kh√≥a t√¨m ki·∫øm. T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng trung b√¨nh gi·ªØa 5 m·∫´u g·ª£i √Ω cho m·ªôt m·∫´u, sau ƒë√≥ √°p d·ª•ng cho 10 c·ª•m t·ª´ tr√™n v√† t√≠nh trung b√¨nh
             ''')
    
    st.markdown('#### 2. K·∫øt qu·∫£')
    st.write('Gi·ªØa 02 m√¥ h√¨nh Gensim v√† Cosine similarity, Cosine similarity, trong c·∫£ 2 tr∆∞·ªùng h·ª£p ch·ªçn xe c√≥ s·∫µn ho·∫∑c t√¨m b·∫±ng t·ª´ kh√≥a, cho ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng trung b√¨nh cao h∆°n so v·ªõi Gensim v√† cho c√°c g·ª£i √Ω s√°t nghƒ©a h∆°n Gensim.\nM√¥ h√¨nh d√πng ƒë·ªÉ d·ª± ƒëo√°n xe trong ·ª©ng d·ª•ng n√†y l√† Cosine similarity.') 

    yellow_pad_header('M√¥ h√¨nh ph√¢n kh√∫c xe m√°y')
    
    st.markdown('#### 1. X·ª≠ l√Ω d·ªØ li·ªáu')
    st.write('D·ªØ li·ªáu ƒë∆∞·ª£c l√†m s·∫°ch, c√°c ƒë·∫∑c tr∆∞ng bi·∫øn s·ªë li√™n t·ª•c nh∆∞ gi√°, kho·∫£ng gi√° th·∫•p nh·∫•t, l·ªõn nh·∫•t, tu·ªïi xe, s·ªë km ƒë√£ ƒëi ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ t·∫°o m√¥ h√¨nh ph√¢n c·ª•m')

    st.markdown('#### 2. Ph√¢n c·ª•m b·∫±ng c√°c ph∆∞∆°ng ph√°p kh√°c nhau')
    st.write('''
    M√¥ h√¨nh ph√¢n c·ª•m ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n 02 m√¥i tr∆∞·ªùng: m√°y h·ªçc truy·ªÅn th·ªëng (sci-kit learn) v√† PySpark.
    - M√°y h·ªçc truy·ªÅn th·ªëng: KMeans, Bisect Kmeans, Agglomerative clustering
    - PySpark: Kmeans, Bisecting Kmeans, GMM.

    ''')

    st.markdown('#### 3. K·∫øt qu·∫£')


    st.markdown('''
    S·ªë c·ª•m ƒë∆∞·ª£c t·∫°o th√†nh tr√™n m√¥ h√¨nh m√°y h·ªçc truy·ªÅn th·ªëng: **03 c·ª•m**
    S·ªë c·ª•m ƒë∆∞·ª£c t·∫°o th√†nh tr√™n PySpark: **02 c·ª•m**''')
    st.image("silhoutte_sklearn.png",width=image_width)                

    st.markdown('''      
    KMeans tr√™n m√¥i tr∆∞·ªùng m√°y h·ªçc truy·ªÅn th·ªëng cho k·∫øt qu·∫£ silhoutte score cao nh·∫•t v√† k·∫øt qu·∫£ ph√¢n c·ª•m d·ªÖ di·ªÖn gi·∫£i h∆°n.
    
    **Ph√¢n lo·∫°i ph√¢n kh√∫c xe**:                
    1/ C·ª•m 0: Ph√¢n kh√∫c Xe Ph·ªï Th√¥ng ‚Äì Trung c·∫•p (Mid-range Popular Motorcycles): Xe tu·ªïi trung b√¨nh, gi√° v·ª´a ph·∫£i, ph√π h·ª£p ƒë·∫°i ƒëa s·ªë ng∆∞·ªùi mua.   
    2/ C·ª•m 1: Ph√¢n kh√∫c Xe Cao C·∫•p ‚Äì Premium / High-end Motorcycles: Ti√™u bi·ªÉu l√† c√°c d√≤ng SH, Vespa cao c·∫•p, ph√¢n kh·ªëi l·ªõn, xe m·ªõi ch·∫°y √≠t.          
    3/ C·ª•m 2: Ph√¢n kh√∫c Xe C≈© ‚Äì Ti·∫øt Ki·ªám (Budget Used Motorcycles): Gi√° r·∫ª nh·∫•t, xe tu·ªïi cao, ch·∫°y nhi·ªÅu ‚Äî ph√π h·ª£p kh√°ch c·∫ßn xe r·∫ª ƒë·ªÉ di chuy·ªÉn c∆° b·∫£n.
    ''')


    st.write('''Trong 3 m√¥ h√¨nh ph√¢n c·ª•m KMeans, Bisect KMeans v√† Agglomerate th√¨ KMeans v·ªõi k = 3 cho k·∫øt qu·∫£ ph√¢n c·ª•m t·ªët nh·∫•t.
            n√™n m√¥ h√¨nh ph√¢n c·ª•m xe ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ·ª©ng d·ª•ng n√†y l√† KMeans v·ªõi k = 3.''')

    st.markdown('#### 4. Th·ªëng k√™ theo t·ª´ng c·ª•m:')

    st.write('Tr·ª±c quan h√≥a')
    st.image('pca_clusters.png')

    cluster_summary = (
        df_cluster.groupby('cluster_label')
        .agg(
            count=('cluster_label', 'size'),
            avg_price=('price', 'mean'),
            avg_age=('age', 'mean'),
            avg_mileage=('mileage_km', 'mean')
        )
        .sort_values('cluster_label')
    )


    # Rename the index (cluster_label ‚Üí Nh√£n c·ª•m xe)
    cluster_summary = cluster_summary.rename_axis("Nh√£n c·ª•m xe")

    # Rename columns
    cluster_summary = cluster_summary.rename(columns={
        "count": "S·ªë l∆∞·ª£ng (xe)",
        "avg_price": "Gi√° trung b√¨nh (VND)",
        "avg_age": "Tu·ªïi trung b√¨nh (nƒÉm)",
        "avg_mileage": "S·ªë km trung b√¨nh (km)"
    })

    # Format s·ªë nguy√™n v√† th√™m d·∫•u ph·∫©y
    cluster_summary["Gi√° trung b√¨nh (VND)"] = (
        cluster_summary["Gi√° trung b√¨nh (VND)"]
            .round(0).astype(int)
            .map(lambda x: f"{x:,}")
    )

    cluster_summary["S·ªë km trung b√¨nh (km)"] = (
        cluster_summary["S·ªë km trung b√¨nh (km)"]
            .round(0).astype(int)
            .map(lambda x: f"{x:,}")
    )

    st.dataframe(cluster_summary, width='stretch')


elif page == "G·ª£i √Ω m·∫´u xe t∆∞∆°ng t·ª±":
    # Main page header
    st.markdown("""
    <h1 style='font-size:35px; font-weight:800; margin-bottom:8px;'>
        G·ª£i √Ω m·∫´u xe t∆∞∆°ng t·ª±
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
    """, unsafe_allow_html=True)

    # Prepare data + vector
    df_clean, X_final = df_clean, X_final

    # Styling and helpers
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)

    st.markdown("""
        <style>
        .card {
            border-radius: 10px;
            padding: 14px 16px;
            margin: 8px 0;
            border: 1px solid #eee;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            background-color: #ffffff;
        }
        .bike-title {
            font-size: 16px;
            font-weight: 700;
            margin-bottom: 4px;
        }
        .bike-sub {
            font-size: 13px;
            color: #666666;
        }
        .small-muted {
            font-size: 12px;
            color: #777777;
        }
        </style>
    """, unsafe_allow_html=True)

    def display_bike_card(row):
        title = row.get('title', 'N/A')
        price = fmt_vnd(row.get('price', None))
        brand = row.get('brand', '-')
        model = row.get('model', '-')
        km = row.get('mileage_km', '-')
        year = row.get('registration_year', '-')
        year_shown = int(year) if str(year).isdigit() else year
        origin = row.get('origin', '-')
        desc = row.get('description', '')

        card_html = f"""
        <div class='card'>
            <div style='display:flex; gap:14px; align-items:center'>
                <div style='flex:1'>
                    <div class='bike-title'>{title}</div>
                    <div class='bike-sub'>{brand} ‚Äî {model} ‚Ä¢ {origin}</div>
                    <div style='margin-top:6px'>{textwrap.shorten(str(desc), width=220)}</div>
                </div>
                <div style='text-align:right; min-width:150px'>
                    <div style='font-weight:700; font-size:16px'>{price}</div>
                    <div class='small-muted' style='margin-top:8px'>
                        S·ªë km: {km}<br/>NƒÉm: {year_shown}
                    </div>
                </div>
            </div>
        </div>
        """
        st.markdown(card_html, unsafe_allow_html=True)

    # ‚úÖ Main interaction
    yellow_pad_header("G·ª£i √Ω theo m·∫´u c√≥ s·∫µn")

    titles_list = df_clean['title'].unique().tolist()
    selected = st.selectbox("Ch·ªçn 1 m·∫´u trong danh s√°ch", titles_list)

    if st.button("G·ª£i √Ω"):
        with st.spinner("üîé ƒêang t√¨m m·∫´u t∆∞∆°ng t·ª±..."):
            df_top, scores = get_top_n_similar_by_content(
                df_clean,
                X_final,
                title=selected,
                top_n=5
            )

        if df_top is None or len(df_top) == 0:
            st.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ‚Äî ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
        else:
            st.success(f"ƒê√£ t√¨m {len(df_top)} m·∫´u t∆∞∆°ng t·ª± ‚úÖ")

            # ‚úÖ Show selected bike
            st.markdown("#### üî∂ M·∫´u b·∫°n ƒë√£ ch·ªçn")
            selected_row = df_clean[df_clean["title"] == selected].iloc[0]
            display_bike_card(selected_row)

            # ‚úÖ Show recommendations
            st.markdown("#### üî∂ C√°c m·∫´u t∆∞∆°ng t·ª±")
            for _, row in df_top.iterrows():
                display_bike_card(row)
                st.caption(f"Similarity score: {row['similarity_score']:.3f}")

        
    # theo t·ª´ kh√≥a
    yellow_pad_header("T√¨m ki·∫øm theo t·ª´ kh√≥a")

    q = st.text_input('Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm, v√≠ d·ª•: "honda vision 2014 m√†u ƒë·ªè"')
    top_k = st.selectbox('S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ', [1, 3, 5, 10])

    if st.button('T√¨m ki·∫øm') and q.strip():
        with st.spinner('ƒêang x·ª≠ l√Ω t·ª´ kh√≥a...'):

            # 1) Clean query like training data
            q_clean = clean_text(q)

            # 2) Vectorize cleaned query
            q_vec_tfidf = vectorizer.transform([q_clean])

            # 3) Pad numeric features with zeros
            num_dim = X_final.shape[1] - q_vec_tfidf.shape[1]
            q_num_zeros = np.zeros((1, num_dim))

            # 4) Combine TF-IDF + numeric zeros
            q_vec = hstack([q_vec_tfidf, q_num_zeros])

            # 5) Compute similarity
            sim_scores = cosine_similarity(q_vec, X_final).flatten()

            # 6) Select top results
            idxs = sim_scores.argsort()[::-1][:top_k]

            # 7) Select rows from cleaned DF
            res_df = df_clean.iloc[idxs].copy()
            res_df['similarity_score'] = sim_scores[idxs]

        st.success(f'K·∫øt qu·∫£ top {top_k} cho: "{q}"')

        # 8) Display
        for _, row in res_df.iterrows():
            display_bike_card(row)
            st.caption(f"Similarity score: {row['similarity_score']:.3f}")


elif page == "X√°c ƒë·ªãnh ph√¢n kh√∫c xe m√°y":
    # Main page header
    st.markdown("""
    <h1 style='font-size:35px; font-weight:800; margin-bottom:8px;'>
        Ph√¢n c·ª•m ph√¢n kh√∫c xe m√°y
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
    """, unsafe_allow_html=True)

    # Yellow pad header function (keep for consistent style)
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)

    # ----- Card CSS -----
    st.markdown("""
        <style>
        .card {
            border-radius: 10px;
            padding: 14px 16px;
            margin: 8px 0;
            border: 1px solid #eee;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            background-color: #ffffff;
        }
        .bike-title {
            font-size: 16px;
            font-weight: 700;
            margin-bottom: 4px;
        }
        .bike-sub {
            font-size: 13px;
            color: #666666;
        }
        .small-muted {
            font-size: 12px;
            color: #777777;
        }
        </style>
    """, unsafe_allow_html=True)

        # ======================== HEADER + CSS ========================
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)


    st.markdown("""
    <style>
    .card {
        border-radius: 10px;
        padding: 14px 16px;
        margin: 8px 0;
        border: 1px solid #eee;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        background-color: #ffffff;
    }
    .cluster-card {
        border-radius: 10px;
        padding: 14px 18px;
        margin: 10px 0;
        border: 1px solid #E5C600;
        box-shadow: 0 2px 4px rgba(0,0,0,0.08);
        color: #000000;
    }
    .cluster-title {
        font-weight: 700;
        font-size: 18px;
        margin-bottom: 6px;
        color: #000000;
    }
    .cluster-desc {
        font-size: 14px;
        color: #000000;
        line-height: 1.4;
    }
    .cluster-0 { background: #FFF7A6; }
    .cluster-1 { background: #FFE970; }
    .cluster-2 { background: #FFDE37; }
    </style>
    """, unsafe_allow_html=True)


    # yellow_pad_header("Ph√¢n t√≠ch & ƒê·ªãnh gi√° theo c·ª•m (Cluster)")


    # ======================== PREDICT CLUSTER ========================

    cluster_names = {
        0: "Ph√¢n kh√∫c Xe Ph·ªï Th√¥ng ‚Äì Trung c·∫•p",
        1: "Ph√¢n kh√∫c Xe Cao C·∫•p ‚Äì Premium",
        2: "Ph√¢n kh√∫c Xe C≈© ‚Äì Ti·∫øt Ki·ªám"
    }

    cluster_cards = {
        0: """
            <div class="cluster-card cluster-0">
                <div class="cluster-title">Ph√¢n kh√∫c Xe Ph·ªï Th√¥ng ‚Äì Trung c·∫•p</div>
                <div class="cluster-desc">
                    Gi√° th·∫•p ‚Äì tu·ªïi xe trung b√¨nh ‚Äì s·ªë km ch·∫°y v·ª´a ph·∫£i.<br>
                    Ph√¢n kh√∫c xe ph·ªï th√¥ng, ph√π h·ª£p ƒëa s·ªë ng∆∞·ªùi mua.
                </div>
            </div>
        """,
        1: """
            <div class="cluster-card cluster-1">
                <div class="cluster-title">Ph√¢n kh√∫c Xe Cao C·∫•p ‚Äì Premium</div>
                <div class="cluster-desc">
                    Xe m·ªõi ‚Äì √≠t km ‚Äì gi√° cao.<br>
                    C√°c d√≤ng SH, Vespa, xe cao c·∫•p, t√¨nh tr·∫°ng t·ªët.
                </div>
            </div>
        """,
        2: """
            <div class="cluster-card cluster-2">
                <div class="cluster-title">Ph√¢n kh√∫c Xe C≈© ‚Äì Ti·∫øt Ki·ªám</div>
                <div class="cluster-desc">
                    Gi√° th·∫•p nh·∫•t ‚Äì km r·∫•t cao ‚Äì tu·ªïi xe l·ªõn.<br>
                    Ph√¢n kh√∫c xe ƒë√£ c≈© ho·∫∑c c√≥ d·∫•u hi·ªáu xu·ªëng c·∫•p.
                </div>
            </div>
        """
    }

    
    # T·∫°o 2 TAB
    tab_user, tab_admin, tab_dash = st.tabs(["User nh·∫≠p tin", "Admin duy·ªát", "Dashboard"])

    # ======================================
    # 1) TAB USER
    # ======================================
    with tab_user:


        # H√†m l∆∞u request user v√†o file Excel
        def save_user_request(df_input, cluster_label):
            save_path = "user_submissions.xlsx"
            
            # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh thay ƒë·ªïi DataFrame g·ªëc (df_in)
            df_save = df_input.copy() 

            # 1. Ki·ªÉm tra xem c·ªôt 'post_time' c√≥ t·ªìn t·∫°i kh√¥ng
            if 'post_time' in df_save.columns:
                # 2. N·∫øu c·ªôt l√† timezone-aware (c√≥ m√∫i gi·ªù), chuy·ªÉn n√≥ th√†nh timezone-unaware
                if df_save['post_time'].dt.tz is not None:
                    # .dt.tz_localize(None) s·∫Ω lo·∫°i b·ªè th√¥ng tin m√∫i gi·ªù (GMT+7)
                    # D·ªØ li·ªáu ng√†y gi·ªù v·∫´n gi·ªØ nguy√™n gi√° tr·ªã theo gi·ªù ƒë·ªãa ph∆∞∆°ng (GMT+7)
                    df_save['post_time'] = df_save['post_time'].dt.tz_localize(None)

            df_save["cluster_label"] = cluster_label
            # df_save["is_outlier"] = 0
            df_save["approved"] = False

            if os.path.exists(save_path):
                old = pd.read_excel(save_path)
                new = pd.concat([old, df_save], ignore_index=True)
            else:
                new = df_save.copy()

            # ƒêo·∫°n n√†y s·∫Ω ch·∫°y tr∆°n tru v√¨ c·ªôt ng√†y gi·ªù ƒë√£ l√† timezone-unaware
            new.to_excel(save_path, index=False)

        # ============================
        # 1.1 Nh·∫≠p tay
        # ============================
        st.subheader("Nh·∫≠p th√¥ng tin xe c·∫ßn rao b√°n")
        col1, col2 = st.columns(2)

        with col1:
            title = st.text_input("Ti√™u ƒë·ªÅ", value='B√°n xe')
            address = st.text_input("ƒê·ªãa ch·ªâ", value= 'Qu·∫≠n 1, TP. HCM')
            brand = st.selectbox("Th∆∞∆°ng hi·ªáu", brand_list)
            model_name = st.selectbox("D√≤ng xe", model_list)
            bike_type = st.selectbox("Lo·∫°i xe", bike_type_list)
            origin = st.selectbox("Xu·∫•t x·ª©", origin_list)
            engine_capacity = st.selectbox("Dung t√≠ch", engine_capacity_list)

        with col2:
            description = st.text_input("M√¥ t·∫£ chi ti·∫øt", value='B√°n xe gi√° r·∫ª')
            registration_year = st.number_input("NƒÉm ƒëƒÉng k√Ω", 1980, 2025, 2019)
            mileage_km = st.number_input("S·ªë km ƒë√£ ƒëi", 0, value=10000)
            min_price = st.number_input("Kho·∫£ng gi√° min", 0)
            max_price = st.number_input("Kho·∫£ng gi√° max", 0)
            price = st.number_input("Gi√° ni√™m y·∫øt", 0, value=20000000)
        
        # Th√™m ng√†y gi·ªù ƒëƒÉng tin
        col_d, col_t = st.columns(2)

        with col_d:
            # B·∫°n c√≥ th·ªÉ gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh l√† gi·ªù hi·ªán t·∫°i
            post_date = st.date_input("Ng√†y ƒëƒÉng tin", value=pd.Timestamp.now(tz=pytz.timezone('Asia/Ho_Chi_Minh')).date())

        with col_t:
            post_time = st.time_input("Gi·ªù ƒëƒÉng tin", value=pd.Timestamp.now(tz=pytz.timezone('Asia/Ho_Chi_Minh')).time())

        # G·ªôp th√†nh datetime v√† g√°n m√∫i gi·ªù:
        # 1. T·∫°o ƒë·ªëi t∆∞·ª£ng datetime th√¥ (naive datetime) t·ª´ date v√† time input
        naive_datetime = pd.to_datetime(str(post_date) + " " + str(post_time))

        # 2. ƒê·ªãnh nghƒ©a m√∫i gi·ªù Asia/Ho_Chi_Minh (GMT+7)
        vietnam_tz = pytz.timezone('Asia/Ho_Chi_Minh')

        # 3. G√°n m√∫i gi·ªù cho ƒë·ªëi t∆∞·ª£ng datetime
        post_datetime = vietnam_tz.localize(naive_datetime)

        # chu·∫©n b·ªã key cho session_state
        if "last_df_in" not in st.session_state:
            st.session_state["last_df_in"] = None
        if "last_anomaly" not in st.session_state:
            st.session_state["last_anomaly"] = None
        if "checked" not in st.session_state:
            st.session_state["checked"] = False

        if st.button("Ki·ªÉm tra"):
            df_in = pd.DataFrame([{
                "title": title,
                "description": description,
                "address": address,
                "brand": brand,
                "model": model_name,
                "bike_type": bike_type,
                "origin": origin,
                "engine_capacity": engine_capacity,
                "registration_year": registration_year,
                "mileage_km": mileage_km,
                "min_price": min_price,
                "max_price": max_price,
                "price": price
            }])

            df_in["age"] = 2025 - df_in["registration_year"]
            df_in["post_time"] = post_datetime

            # Mapping using helpers
            if helpers is not None:
                if df_in.at[0, 'brand'] in helpers['rare_brands']:
                    df_in['brand_grouped'] = 'H√£ng kh√°c'
                else:
                    df_in['brand_grouped'] = df_in['brand']

                rare_models = helpers['model_group_maps'].get(df_in.at[0, 'brand_grouped'], set())
                if df_in.at[0, 'model'] in rare_models:
                    df_in['model_grouped'] = 'D√≤ng kh√°c'
                else:
                    df_in['model_grouped'] = df_in['model']

                df_in["segment"] = df_in["brand_grouped"] + "_" + df_in["model_grouped"]
                df_in["brand_meanprice"] = helpers["brand_mean_map"].get(df_in.at[0,"brand"], np.nan)
            else:
                df_in["brand_grouped"] = df_in["brand"]
                df_in["model_grouped"] = df_in["model"]
                df_in["segment"] = df_in["brand"] + "_" + df_in["model"]
                df_in["brand_meanprice"] = np.nan

            try:
                df_all, anomaly = detect_outliers(df_in, model_path=MODEL_PATH, input_is_df=True, helpers=helpers)

                # l∆∞u t·∫°m v√†o session ƒë·ªÉ d√πng sau khi user x√°c nh·∫≠n
                st.session_state["last_df_in"] = df_in
                st.session_state["last_anomaly"] = anomaly
                st.session_state["checked"] = True

            except Exception as e:
                st.exception(e)

        # N·∫øu ƒë√£ c√≥ k·∫øt qu·∫£ ki·ªÉm tra trong session_state th√¨ hi·ªÉn th·ªã
        if st.session_state.get("checked", False):
            df_in = st.session_state["last_df_in"]
            anomaly = st.session_state["last_anomaly"]

            if anomaly is None:
                st.info("Kh√¥ng c√≥ k·∫øt qu·∫£ ki·ªÉm tra.")
            else:
                if len(anomaly) > 0:
                    # x√°c ƒë·ªãnh reason d·ª±a tr√™n score nh∆∞ y√™u c·∫ßu (model/business)
                    # note: detect_outliers ƒë√£ t√≠nh score_model_based, score_business_based
                    r = []

                    price = anomaly["price"].iloc[0]
                    resid = anomaly["resid"].iloc[0]
                    p10 = anomaly["p10"].iloc[0]
                    p90 = anomaly["p90"].iloc[0]

                    # T√≠nh gi√° m√¥ h√¨nh d·ª± ƒëo√°n
                    predicted_price = price - resid
                    if predicted_price > 0:
                        diff_pct = resid / predicted_price * 100
                    else:
                        diff_pct = None


                    # ===================================================
                    # 1) L√ù DO D·ª∞A TR√äN ƒêI·ªÇM M√î H√åNH (score_model_based)
                    # ===================================================
                    # if anomaly["score_model_based"].iloc[0] >= 50:
                    #     r.append("M√¥ h√¨nh ƒë√°nh gi√° xe c√≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng")

                    # 1.1) Residual Z-score ‚Äì gi√° l·ªách xa m√¥ h√¨nh d·ª± ƒëo√°n
                    if anomaly["flag_resid"].iloc[0] == 1:
                        if diff_pct is not None:
                            if resid > 0:
                                r.append(
                                    f"Gi√° ƒëang CAO h∆°n m·ª©c m√¥ h√¨nh d·ª± ƒëo√°n kho·∫£ng {diff_pct:.1f}%"
                                )
                            else:
                                r.append(
                                    f"Gi√° ƒëang TH·∫§P h∆°n m·ª©c m√¥ h√¨nh d·ª± ƒëo√°n kho·∫£ng {abs(diff_pct):.1f}%"
                                )
                        else:
                            r.append("Gi√° l·ªách qu√° xa m√¥ h√¨nh d·ª± ƒëo√°n")

                    # 1.2) Gi√° n·∫±m ngo√†i kho·∫£ng Min‚ÄìMax
                    if anomaly["flag_minmax"].iloc[0] == 1:
                        r.append("Gi√° n·∫±m ngo√†i kho·∫£ng gi√° h·ª£p l√Ω (Min‚ÄìMax)")

                    # 1.3) Gi√° n·∫±m ngo√†i ph√¢n v·ªã P10‚ÄìP90
                    if anomaly["flag_p10p90"].iloc[0] == 1:
                        if price < p10:
                            r.append("Gi√° thu·ªôc nh√≥m 10% TH·∫§P NH·∫§T c·ªßa ph√¢n kh√∫c (r·∫ª b·∫•t th∆∞·ªùng)")
                        elif price > p90:
                            r.append("Gi√° thu·ªôc nh√≥m 10% CAO NH·∫§T c·ªßa ph√¢n kh√∫c (cao b·∫•t th∆∞·ªùng)")
                        else:
                            r.append("Gi√° n·∫±m ngo√†i kho·∫£ng P10‚ÄìP90 c·ªßa ph√¢n kh√∫c")

                    # 1.4) B·∫•t th∆∞·ªùng t·ª´ m√¥ h√¨nh kh√¥ng gi√°m s√°t (Isolation Forest, LOF, KMeans)
                    if anomaly["flag_unsup"].iloc[0] == 1:
                        r.append("M√¥ h√¨nh h·ªçc m√°y kh√¥ng gi√°m s√°t ph√°t hi·ªán ƒëi·ªÉm b·∫•t th∆∞·ªùng")


                    # ===================================================
                    # 2) L√ù DO THEO LOGIC NGHI·ªÜP V·ª§ (score_business_based)
                    # ===================================================
                    if anomaly["flag_mileage_low"].iloc[0] == 1:
                        r.append("S·ªë km ƒë√£ ƒëi TH·∫§P b·∫•t th∆∞·ªùng so v·ªõi tu·ªïi xe")

                    if anomaly["flag_mileage_high"].iloc[0] == 1:
                        r.append("S·ªë km ƒë√£ ƒëi CAO b·∫•t th∆∞·ªùng so v·ªõi tu·ªïi xe")


                    # ===================================================
                    # 3) X·ª¨ L√ù K·∫æT QU·∫¢ CU·ªêI
                    # ===================================================
                    # reason_text = " + ".join(r) if r else "Kh√¥ng x√°c ƒë·ªãnh nguy√™n nh√¢n"

                    st.error("üö® H·ªá th·ªëng ph√°t hi·ªán b√†i ƒëƒÉng c√≥ d·∫•u hi·ªáu B·∫§T TH∆Ø·ªúNG")

                    if r:
                        st.markdown(
                            "**Nguy√™n nh√¢n chi ti·∫øt:**\n"
                            + "\n".join([f"- {reason}" for reason in r])
                        )
                    else:
                        st.markdown("Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c nguy√™n nh√¢n.")
                    # st.dataframe(anomaly)

                     # ======================================
                    # CLUSTER + TH√îNG B√ÅO CHI TI·∫æT
                    # ======================================

                    # ======================== T√çNH TO√ÅN ========================

                    try:
                        # L·∫•y ƒë√∫ng scaler v√† model cluster
                        scaler = helpers["cluster_scaler"]
                        kmeans = helpers["cluster_model"]
                        cluster_summary = helpers["cluster_summary"]

                        # T·∫°o log_price n·∫øu ch∆∞a c√≥
                        if "log_price" not in df_in.columns:
                            df_in["log_price"] = np.log1p(df_in["price"])

                        # Chu·∫©n ho√° nh∆∞ l√∫c train
                        X_cluster = df_in[["age","mileage_km","min_price","max_price","log_price"]]
                        X_cluster_scaled = scaler.transform(X_cluster)

                        # D·ª± ƒëo√°n c·ª•m
                        cluster_label = int(kmeans.predict(X_cluster_scaled)[0])

                        # L·∫•y gi√° trung b√¨nh c·ª•m
                        # N·∫øu summary l√† DataFrame
                        if hasattr(cluster_summary, "index"):
                            if cluster_label in cluster_summary.index:
                                cluster_mean_price = cluster_summary.loc[cluster_label, "avg_price"]
                            else:
                                cluster_mean_price = None
                        else:
                            # N·∫øu summary l√† dict
                            if cluster_label in cluster_summary:
                                cluster_mean_price = cluster_summary[cluster_label]["avg_price"]
                            else:
                                cluster_mean_price = None

                        price = df_in["price"].iloc[0]
                        if cluster_mean_price and cluster_mean_price > 0:
                            diff_pct = (price - cluster_mean_price) / cluster_mean_price * 100
                            diff_vnd = price - cluster_mean_price
                        else:
                            diff_pct = None
                            diff_vnd = None

                    except Exception as e:
                        st.error(f"Cluster error: {e}")
                        cluster_label = None
                        diff_pct = None
                        diff_vnd = None
                        cluster_mean_price = None



                    # ======================== HI·ªÇN TH·ªä K·∫æT QU·∫¢ ========================

                    # st.success("üéâ **ƒêƒÉng tin th√†nh c√¥ng!**")
                    st.markdown("### üîé **Ph√¢n lo·∫°i ph√¢n kh√∫c**")

                    # T√™n c·ª•m d·ªÖ hi·ªÉu
                    if cluster_label is not None:
                        st.markdown(f"- **Xe c·ªßa b·∫°n** thu·ªôc **{cluster_names[cluster_label]}**")

                    # Hi·ªÉn th·ªã card m√¥ t·∫£ c·ª•m
                    if cluster_label is not None:
                        st.markdown(cluster_cards[cluster_label], unsafe_allow_html=True)

                    # Ch√™nh l·ªách gi√° so v·ªõi trung b√¨nh c·ª•m
                    if diff_pct is not None:
                        if diff_vnd >= 0:
                            st.markdown(
                                f"- **Gi√° cao h∆°n trung b√¨nh ph√¢n kh√∫c** {diff_pct:.1f}% (**+{diff_vnd:,.0f} VND**)"
                            )
                        else:
                            st.markdown(
                                f"- **Gi√° th·∫•p h∆°n trung b√¨nh ph√¢n kh√∫c** {abs(diff_pct):.1f}% (**{diff_vnd:,.0f} VND**)"
                            )
                    else:
                        st.markdown("- Kh√¥ng c√≥ d·ªØ li·ªáu trung b√¨nh c·ª•m ƒë·ªÉ so s√°nh.")

                    # h·ªèi user: c√≥ mu·ªën ƒëƒÉng kh√¥ng? + n√∫t x√°c nh·∫≠n l∆∞u
                    choice = st.radio("Xe n√†y b·∫•t th∆∞·ªùng, b·∫°n v·∫´n mu·ªën ƒëƒÉng tin kh√¥ng?", ["Kh√¥ng", "C√≥"], horizontal=True, key="confirm_post_radio")

                    if st.button("X√°c nh·∫≠n"):
                        if choice == "C√≥":

                            st.info("üìå Tin ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o h·ªá th·ªëng.")

                            save_user_request(df_in, cluster_names[cluster_label])

                else:
                    st.success("Th√¥ng tin ƒëƒÉng h·ª£p l·ªá.")

                    # ======================================
                    # CLUSTER + TH√îNG B√ÅO CHI TI·∫æT
                    # ======================================

                    # ======================== T√çNH TO√ÅN ========================

                    try:
                        # L·∫•y ƒë√∫ng scaler v√† model cluster
                        scaler = helpers["cluster_scaler"]
                        kmeans = helpers["cluster_model"]
                        cluster_summary = helpers["cluster_summary"]

                        # T·∫°o log_price n·∫øu ch∆∞a c√≥
                        if "log_price" not in df_in.columns:
                            df_in["log_price"] = np.log1p(df_in["price"])

                        # Chu·∫©n ho√° nh∆∞ l√∫c train
                        X_cluster = df_in[["age","mileage_km","min_price","max_price","log_price"]]
                        X_cluster_scaled = scaler.transform(X_cluster)

                        # D·ª± ƒëo√°n c·ª•m
                        cluster_label = int(kmeans.predict(X_cluster_scaled)[0])

                        # L·∫•y gi√° trung b√¨nh c·ª•m
                        # N·∫øu summary l√† DataFrame
                        if hasattr(cluster_summary, "index"):
                            if cluster_label in cluster_summary.index:
                                cluster_mean_price = cluster_summary.loc[cluster_label, "avg_price"]
                            else:
                                cluster_mean_price = None
                        else:
                            # N·∫øu summary l√† dict
                            if cluster_label in cluster_summary:
                                cluster_mean_price = cluster_summary[cluster_label]["avg_price"]
                            else:
                                cluster_mean_price = None

                        price = df_in["price"].iloc[0]
                        if cluster_mean_price and cluster_mean_price > 0:
                            diff_pct = (price - cluster_mean_price) / cluster_mean_price * 100
                            diff_vnd = price - cluster_mean_price
                        else:
                            diff_pct = None
                            diff_vnd = None

                    except Exception as e:
                        st.error(f"Cluster error: {e}")
                        cluster_label = None
                        diff_pct = None
                        diff_vnd = None
                        cluster_mean_price = None



                    # ======================== HI·ªÇN TH·ªä K·∫æT QU·∫¢ ========================

                    # st.success("üéâ **ƒêƒÉng tin th√†nh c√¥ng!**")
                    st.markdown("### üîé **Ph√¢n lo·∫°i ph√¢n kh√∫c**")

                    # T√™n c·ª•m d·ªÖ hi·ªÉu
                    if cluster_label is not None:
                        st.markdown(f"- **Xe c·ªßa b·∫°n** thu·ªôc **{cluster_names[cluster_label]}**")

                    # Hi·ªÉn th·ªã card m√¥ t·∫£ c·ª•m
                    if cluster_label is not None:
                        st.markdown(cluster_cards[cluster_label], unsafe_allow_html=True)

                    # Ch√™nh l·ªách gi√° so v·ªõi trung b√¨nh c·ª•m
                    if diff_pct is not None:
                        if diff_vnd >= 0:
                            st.markdown(
                                f"- **Gi√° cao h∆°n trung b√¨nh ph√¢n kh√∫c** {diff_pct:.1f}% (**+{diff_vnd:,.0f} VND**)"
                            )
                        else:
                            st.markdown(
                                f"- **Gi√° th·∫•p h∆°n trung b√¨nh ph√¢n kh√∫c** {abs(diff_pct):.1f}% (**{diff_vnd:,.0f} VND**)"
                            )
                    else:
                        st.markdown("- Kh√¥ng c√≥ d·ªØ li·ªáu trung b√¨nh c·ª•m ƒë·ªÉ so s√°nh.")

                    # Show n√∫t l∆∞u n·∫øu user mu·ªën (optional) ‚Äî t·ª± l∆∞u ho·∫∑c cho user b·∫•m
                    if st.button("ƒêƒÉng tin"):


                        st.info("üìå Tin ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o h·ªá th·ªëng.")

                        save_user_request(df_in,cluster_names[cluster_label])


    # ======================================
    # 2) TAB ADMIN 
    # ======================================
    with tab_admin:

        st.subheader("Ch·∫ø ƒë·ªô ki·ªÉm tra d√†nh cho Admin")

        mode_admin = st.radio(
            "Ch·ªçn c√°ch ki·ªÉm tra:",
            ["D·ªØ li·ªáu user nh·∫≠p h√¥m nay", "Upload file"],
            horizontal=True
        )
        # ============================================================
        # MODE 1: KI·ªÇM TRA D·ªÆ LI·ªÜU USER NH·∫¨P H√îM NAY
        # ============================================================
        # =========================
        # DUY·ªÜT TIN USER SUBMISSIONS
        # =========================
        if mode_admin == "D·ªØ li·ªáu user nh·∫≠p h√¥m nay":

            # === DUY·ªÜT TIN: D·ªÆ LI·ªÜU USER NH·∫¨P H√îM NAY ===
            save_path = "user_submissions.xlsx"
            system_path = "data_motobikes_realtime.xlsx"
        
            # mapping t·ª´ header submissions (VN) -> h·ªá th·ªëng (EN)
            column_map = {
                'Ti√™u ƒë·ªÅ': 'title',
                'ƒê·ªãa ch·ªâ': 'address',
                'M√¥ t·∫£ chi ti·∫øt': 'description',
                'Gi√°': 'price',
                'Kho·∫£ng gi√° min': 'min_price',
                'Kho·∫£ng gi√° max': 'max_price',
                'Th∆∞∆°ng hi·ªáu': 'brand',
                'D√≤ng xe': 'model',
                'NƒÉm ƒëƒÉng k√Ω': 'registration_year',
                'S·ªë Km ƒë√£ ƒëi': 'mileage_km',
                'T√¨nh tr·∫°ng': 'condition',
                'Lo·∫°i xe': 'bike_type',
                'Dung t√≠ch xe': 'engine_capacity',
                'Xu·∫•t x·ª©': 'origin',
                'Ch√≠nh s√°ch b·∫£o h√†nh': 'warranty_policy',
                'Tr·ªçng l∆∞·ª£ng': 'weight',
                'is_outlier' : 'is_outlier',
                'cluster_label' : 'cluster_label'
            }
            save_path = "user_submissions.xlsx"


            st.subheader("Danh s√°ch tin user ƒë√£ g·ª≠i")

            if os.path.exists(save_path):
                df_user = pd.read_excel(save_path)

                cols_to_hide = ["brand_grouped", "model_grouped", "segment", "brand_meanprice","tmp_id"]
                df_user_display = df_user.drop(columns=[c for c in cols_to_hide if c in df_user.columns])

                st.dataframe(df_user_display.sort_values(by='post_time', ascending=False))

                # --- n√∫t: ch·∫°y ki·ªÉm tra b·∫•t th∆∞·ªùng ---
                # --- n√∫t: ch·∫°y ki·ªÉm tra b·∫•t th∆∞·ªùng ---
                if st.button("Ch·∫°y ki·ªÉm tra b·∫•t th∆∞·ªùng (User submissions)"):
                    try:
                        # --- B∆Ø·ªöC 0: ƒë·∫£m b·∫£o df_user l√† b·∫£n copy v√† index ·ªïn ƒë·ªãnh ---
                        df_user = df_user.copy()  # tr√°nh side-effect v·ªõi bi·∫øn g·ªëc ngo√†i scope
                        # reset index ƒë·ªÉ index = 0..N-1 (ƒë·∫£m b·∫£o mapping theo v·ªã tr√≠)
                        df_user = df_user.reset_index(drop=True)

                        # --- B∆Ø·ªöC 1: ƒë·∫£m b·∫£o tmp_id t·ªìn t·∫°i v√† kh√¥ng null (tmp_id = index) ---
                        # L∆∞u √Ω: tmp_id l√† "temporary id" d√πng ƒë·ªÉ mapping ‚Äî form 0..N-1 (match index)
                        df_user["tmp_id"] = df_user.index.astype(int)

                        # --- B∆Ø·ªöC 2: ƒë·∫£m b·∫£o is_outlier m·∫∑c ƒë·ªãnh = 0 ---
                        df_user["is_outlier"] = 0

                        # (t√πy: l∆∞u t·∫°m tmp_id v√†o file ƒë·ªÉ persist n·∫øu mu·ªën)
                        # df_user.to_excel(save_path, index=False)

                        # --- B∆Ø·ªöC 3: g·ªçi h√†m detect_outliers (n√≥ n√™n ch·∫•p nh·∫≠n df c√≥ tmp_id) ---
                        df_all, anomaly = detect_outliers(
                            df_user,
                            model_path=MODEL_PATH,
                            input_is_df=True,
                            helpers=helpers
                        )

                        # b·∫£o ƒë·∫£m anomaly l√† DataFrame (tr√°nh None)
                        if anomaly is None:
                            anomaly = df_all.iloc[0:0].copy()

                        # --- B∆Ø·ªöC 4: n·∫øu anomaly kh√¥ng c√≥ tmp_id, sinh tmp_id t·ª´ index t∆∞∆°ng ·ª©ng ---
                        if "tmp_id" not in anomaly.columns:
                            # reset index of anomaly to ensure it's aligned with df_all/df_user positions
                            anomaly = anomaly.reset_index(drop=False)  # keep old index in column 'index' if needed
                            # n·∫øu anomaly came from df_all where indices align with df_user after reset, then:
                            try:
                                # Try to take tmp_id from df_user using anomaly.index (prefer original index before reset)
                                # If anomaly.index aligns with df_user.index:
                                anomaly["tmp_id"] = df_user.loc[anomaly.index, "tmp_id"].values
                            except Exception:
                                # Fallback robust method: if anomaly has a column 'index' (from reset_index), use that
                                if 'index' in anomaly.columns:
                                    try:
                                        anomaly["tmp_id"] = df_user.loc[anomaly["index"].astype(int), "tmp_id"].values
                                    except Exception:
                                        # If still fails, create tmp_id from anomaly position (last-resort)
                                        anomaly = anomaly.reset_index(drop=True)
                                        anomaly["tmp_id"] = anomaly.index.astype(int)
                                        st.warning("‚ö† Kh√¥ng th·ªÉ map anomaly index tr·ª±c ti·∫øp t·ªõi df_user; ƒë√£ g√°n tmp_id t·∫°m theo v·ªã tr√≠ anomaly (kh·∫£ nƒÉng mapping sai).")
                                else:
                                    anomaly = anomaly.reset_index(drop=True)
                                    anomaly["tmp_id"] = anomaly.index.astype(int)
                                    st.warning("‚ö† Kh√¥ng th·ªÉ map anomaly index tr·ª±c ti·∫øp t·ªõi df_user; ƒë√£ g√°n tmp_id t·∫°m theo v·ªã tr√≠ anomaly (kh·∫£ nƒÉng mapping sai).")

                        # --- B∆Ø·ªöC 5: map anomaly -> df_user ƒë·ªÉ set is_outlier = 1 ch√≠nh x√°c ---
                        try:
                            # preferred: map b·∫±ng tmp_id (anomaly["tmp_id"] ch·ª©a tmp_id t∆∞∆°ng ·ª©ng c·ªßa df_user)
                            matched_tmp = set(anomaly["tmp_id"].astype(int).tolist())
                            df_user["is_outlier"] = df_user["tmp_id"].apply(lambda x: 1 if int(x) in matched_tmp else 0)
                        except Exception:
                            # fallback: th·ª≠ map b·∫±ng index n·∫øu tmp_id mapping fail
                            try:
                                df_user["is_outlier"] = 0
                                df_user.loc[anomaly.index, "is_outlier"] = 1
                                st.warning("‚ö† ƒê√£ d√πng fallback mapping theo index ƒë·ªÉ ƒë√°nh d·∫•u is_outlier.")
                            except Exception:
                                df_user["is_outlier"] = 0
                                st.warning("‚ö† Kh√¥ng th·ªÉ map anomaly ƒë·ªÉ g√°n is_outlier ‚Äî t·∫•t c·∫£ gi·ªØ 0.")

                        # --- B∆Ø·ªöC 6: l∆∞u l·∫°i file submissions (persist tmp_id + is_outlier) ---
                        df_user.to_excel(save_path, index=False)

                        st.success(f"Ph√°t hi·ªán {len(anomaly)} tin b·∫•t th∆∞·ªùng")

                        # hi·ªÉn th·ªã anomaly r√∫t g·ªçn (b·ªè c√°c c·ªôt n·ªôi b·ªô n·∫øu c√≥)
                        cols_drop = [
                            'brand_grouped','model_grouped','segment','brand_meanprice',
                            'price_hat','resid','resid_median','resid_std','resid_z','flag_resid',
                            'p10','p90','log_price'
                        ]
                        st.dataframe(anomaly.drop(columns=[c for c in cols_drop if c in anomaly.columns], errors='ignore').head(20))

                    except Exception as e:
                        st.error("‚ùå L·ªói khi ch·∫°y ki·ªÉm tra b·∫•t th∆∞·ªùng")
                        st.exception(e)


                        # === B·∫ÆT ƒê·∫¶U TH√äM N√öT T·∫¢I XU·ªêNG ===
                        if len(anomaly) > 0:
                            # 1. T·∫°o t√™n file c√≥ ng√†y gi·ªù
                            now = datetime.now().strftime("%Y%m%d_%H%M%S")
                            file_name = f"anomaly_detection_user_{now}.csv"
                            
                            # 2. Chuy·ªÉn DataFrame sang CSV
                            # Lo·∫°i b·ªè m√∫i gi·ªù kh·ªèi c·ªôt 'post_time' tr∆∞·ªõc khi t·∫£i xu·ªëng n·∫øu c·∫ßn (ƒë·∫£m b·∫£o kh√¥ng l·ªói)
                            df_output = anomaly.copy()
                            if 'post_time' in df_output.columns and df_output['post_time'].dt.tz is not None:
                                df_output['post_time'] = df_output['post_time'].dt.tz_localize(None)

                            csv = df_output.to_csv(index=False).encode('utf-8')
                            
                            # 3. T·∫°o n√∫t t·∫£i xu·ªëng
                            st.download_button(
                                label="T·∫£i k·∫øt qu·∫£ b·∫•t th∆∞·ªùng (CSV)",
                                data=csv,
                                file_name=file_name,
                                mime='text/csv'
                            )
                        # === K·∫æT TH√öC TH√äM N√öT T·∫¢I XU·ªêNG ===

                    except Exception as e:
                        st.exception(e)

            else:
                st.info("‚ö† Ch∆∞a c√≥ user n√†o g·ª≠i d·ªØ li·ªáu.")

            # DUY·ªÜT

            # load submissions & h·ªá th·ªëng (an empty DF n·∫øu ch∆∞a c√≥)
            df_user = pd.read_excel(save_path) if os.path.exists(save_path) else pd.DataFrame()
            df_system = pd.read_excel(system_path) if os.path.exists(system_path) else pd.DataFrame(columns=list(column_map.values()) + ["id"])

            st.markdown("### üìù Duy·ªát tin c·ªßa user")

            if df_user.empty:
                st.info("Hi·ªán ch∆∞a c√≥ tin t·ª´ user.")
            else:
                df_user = df_user.copy()

                # ---- tmp_id ƒë·ªÉ tracking (t·∫°m th·ªùi, persistent trong file cho mapping detect_outliers) ----
                if "tmp_id" not in df_user.columns:
                    df_user.insert(0, "tmp_id", range(1, len(df_user) + 1))
                else:
                    # ƒë·∫£m b·∫£o tmp_id li√™n t·ª•c n·∫øu c·∫ßn
                    df_user["tmp_id"] = range(1, len(df_user) + 1)

                # init c·ªôt approved/is_outlier n·∫øu ch∆∞a c√≥
                if "approved" not in df_user.columns:
                    df_user["approved"] = False
                if "is_outlier" not in df_user.columns:
                    df_user["is_outlier"] = 0

                # --- th·ªëng k√™ ch·ªù duy·ªát ---
                df_pending = df_user[df_user["approved"] == False].copy()
                num_total = len(df_pending)
                num_outlier = int(df_pending["is_outlier"].sum()) if "is_outlier" in df_pending.columns else 0
                num_normal = num_total - num_outlier
                st.info(f"üìå T·ªïng {num_total} tin ch∆∞a duy·ªát: **{num_normal} tin h·ª£p l·ªá**, **{num_outlier} tin b·∫•t th∆∞·ªùng**.")

                # --- option hi·ªÉn th·ªã (ch·ªâ 1 ƒë∆∞·ª£c ch·ªçn) ---
                option = st.radio(
                    "Ch·∫ø ƒë·ªô hi·ªÉn th·ªã (ch·ªâ ch·ªçn 1):",
                    ["Ch·ªâ hi·ªán tin h·ª£p l·ªá", "Ch·ªâ hi·ªán tin b·∫•t th∆∞·ªùng", "Hi·ªÉn th·ªã t·∫•t c·∫£"]
                )

                df_display = df_pending.copy()
                if option == "Ch·ªâ hi·ªán tin h·ª£p l·ªá":
                    df_display = df_display[df_display["is_outlier"] == 0].copy()
                elif option == "Ch·ªâ hi·ªán tin b·∫•t th∆∞·ªùng":
                    df_display = df_display[df_display["is_outlier"] == 1].copy()
                # else: gi·ªØ nguy√™n (hi·ªÉn th·ªã t·∫•t c·∫£)

                # Checkbox ch·ªçn duy·ªát t·∫•t c·∫£ trong b·∫£ng hi·ªÉn th·ªã
                select_all = st.checkbox("Ch·ªçn duy·ªát t·∫•t c·∫£ trong b·∫£ng hi·ªÉn th·ªã", value=False)
                df_display["duyet"] = select_all

                # Hi·ªÉn th·ªã data_editor (tmp_id ƒë∆∞·ª£c hi·ªÉn th·ªã ƒë·ªÉ tracking)
                # Hi·ªÉn th·ªã b·∫£ng duy·ªát (·∫©n approved)
                edited_df = st.data_editor(
                    df_display.drop(columns=["approved"], errors="ignore"),
                    use_container_width=True,
                    num_rows="dynamic",
                    column_config={
                        "duyet": st.column_config.CheckboxColumn("Duy·ªát?", default=False),
                        "tmp_id": None  # ·∫©n nh∆∞ng gi·ªØ ƒë·ªÉ map
                    },
                    hide_index=True
                )

                # N·∫øu ch·ªçn duy·ªát t·∫•t c·∫£ ‚Üí override k·∫øt qu·∫£ cu·ªëi c√πng
                if select_all:
                    edited_df["duyet"] = True

                # --- Khi b·∫•m DUY·ªÜT ---
                # --- Khi b·∫•m DUY·ªÜT ---
                if st.button("‚úî Duy·ªát v√† th√™m v√†o h·ªá th·ªëng"):
                    try:
                        # edited_df l√† dataframe tr·∫£ v·ªÅ t·ª´ st.data_editor (ch·ª©a c·ªôt 'duyet' & 'tmp_id')
                        df_selected = edited_df[edited_df["duyet"] == True].copy()

                        if df_selected.empty:
                            st.warning("‚ö† B·∫°n ch∆∞a ch·ªçn tin n√†o ƒë·ªÉ duy·ªát.")
                        else:
                            # L·∫•y list tmp_id ƒë√£ approve (tr∆∞·ªõc khi drop)
                            approved_tmp_ids = df_selected["tmp_id"].tolist()

                            # T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω map/append
                            df_approve_raw = df_selected.drop(columns=["duyet"], errors="ignore").copy()

                            # --- X√ÅC ƒê·ªäNH MAPPING HEADER ---
                            # column_map: VN -> EN (n·∫øu b·∫°n c√≥ mapping kh√°c th√¨ c·∫≠p nh·∫≠t)
                            column_map_vn_to_en = {
                                'Ti√™u ƒë·ªÅ': 'title',
                                'ƒê·ªãa ch·ªâ': 'address',
                                'M√¥ t·∫£ chi ti·∫øt': 'description',
                                'Gi√°': 'price',
                                'Kho·∫£ng gi√° min': 'min_price',
                                'Kho·∫£ng gi√° max': 'max_price',
                                'Th∆∞∆°ng hi·ªáu': 'brand',
                                'D√≤ng xe': 'model',
                                'NƒÉm ƒëƒÉng k√Ω': 'registration_year',
                                'S·ªë Km ƒë√£ ƒëi': 'mileage_km',
                                'T√¨nh tr·∫°ng': 'condition',
                                'Lo·∫°i xe': 'bike_type',
                                'Dung t√≠ch xe': 'engine_capacity',
                                'Xu·∫•t x·ª©': 'origin',
                                'Ch√≠nh s√°ch b·∫£o h√†nh': 'warranty_policy',
                                'Tr·ªçng l∆∞·ª£ng': 'weight',
                                'is_outlier': 'is_outlier',
                                'cluster_label' : 'cluster_label'
                            }
                            # reverse map (EN -> VN)
                            column_map_en_to_vn = {v:k for k,v in column_map_vn_to_en.items()}

                            # System columns (the final Excel has Vietnamese header order you specified)
                            system_cols = [
                                "id",
                                "Ti√™u ƒë·ªÅ", "Gi√°", "Kho·∫£ng gi√° min", "Kho·∫£ng gi√° max",
                                "ƒê·ªãa ch·ªâ", "M√¥ t·∫£ chi ti·∫øt", "Th∆∞∆°ng hi·ªáu", "D√≤ng xe",
                                "NƒÉm ƒëƒÉng k√Ω", "S·ªë Km ƒë√£ ƒëi", "T√¨nh tr·∫°ng", "Lo·∫°i xe",
                                "Dung t√≠ch xe", "Xu·∫•t x·ª©", "Ch√≠nh s√°ch b·∫£o h√†nh",
                                "Tr·ªçng l∆∞·ª£ng", "Href","is_outlier","cluster_label"
                            ]

                            # T·∫°o target df v·ªõi ƒë√∫ng s·ªë d√≤ng (len(df_approve_raw))
                            df_target = pd.DataFrame(index=df_approve_raw.index, columns=system_cols)
                            # ban ƒë·∫ßu r·ªóng -> s·∫Ω g√°n t·ª´ df_approve_raw n·∫øu c√≥

                            # C√≥ th·ªÉ submissions d√πng EN headers (title, price, ...) ho·∫∑c VN headers.
                            # Duy·ªát c√°c c·ªôt trong df_approve_raw v√† map v√†o df_target t∆∞∆°ng ·ª©ng:
                            for col in df_approve_raw.columns:
                                # n·∫øu c·ªôt l√† ti·∫øng Anh v√† c√≥ map sang VN
                                if col in column_map_en_to_vn:
                                    vn_col = column_map_en_to_vn[col]
                                    if vn_col in df_target.columns:
                                        df_target[vn_col] = df_approve_raw[col].values
                                # n·∫øu c·ªôt l√† ti·∫øng Vi·ªát v√† c≈©ng n·∫±m trong system_cols
                                elif col in df_target.columns:
                                    df_target[col] = df_approve_raw[col].values
                                else:
                                    # c·ªôt kh√°c (v√≠ d·ª•: brand_grouped, segment, price_hat, tmp_id, approved, is_outlier)
                                    # n·∫øu c√≥ 'title' ho·∫∑c 'Ti√™u ƒë·ªÅ' t∆∞∆°ng ƒë∆∞∆°ng trong t√™n, ∆∞u ti√™n map
                                    # else: ignore / could log
                                    pass

                            # N·∫øu m·ªôt s·ªë c·ªôt h·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c g√°n, ƒë·ªÉ chu·ªói r·ªóng / NaN -> thay b·∫±ng chu·ªói r·ªóng
                            df_target = df_target.fillna("")

                            # T·∫°o ID auto (theo df_system hi·ªán t·∫°i)
                            next_id = int(df_system["id"].max()) + 1 if (not df_system.empty and "id" in df_system.columns and pd.notna(df_system["id"].max())) else 1
                            df_target["id"] = range(next_id, next_id + len(df_target))

                            # Reorder columns ƒë·ªÉ 'id' l√† c·ªôt ƒë·∫ßu
                            df_target = df_target[system_cols]

                            # Append v√†o h·ªá th·ªëng
                            if os.path.exists(system_path):
                                df_system_existing = pd.read_excel(system_path)
                                df_new_system = pd.concat([df_system_existing, df_target], ignore_index=True)
                            else:
                                df_new_system = df_target.copy()

                            df_new_system.to_excel(system_path, index=False)

                            # --- C·∫¨P NH·∫¨T TR·∫†NG TH√ÅI ƒê√É DUY·ªÜT TRONG user_submissions.xlsx ---
                            # Chuy·ªÉn flag 'approved' cho nh·ªØng tmp_id ƒë√£ duy·ªát
                            df_user.loc[df_user["tmp_id"].isin(approved_tmp_ids), "approved"] = True
                            # L∆∞u l·∫°i file submissions (persist)
                            df_user.to_excel(save_path, index=False)

                            st.success(f"üéâ ƒê√£ duy·ªát v√† th√™m {len(df_target)} tin v√†o d·ªØ li·ªáu h·ªá th·ªëng!")

                    except Exception as e:
                        st.error("‚ùå L·ªói khi duy·ªát d·ªØ li·ªáu.")
                        st.exception(e)


        # ============================================================
        # MODE 2: ADMIN UPLOAD FILE KI·ªÇM TRA
        # ============================================================
        else:
            st.subheader("Upload file ƒë·ªÉ Admin ki·ªÉm tra")

            file_admin = st.file_uploader(
                "Ch·ªçn file d·ªØ li·ªáu c·∫ßn ki·ªÉm tra (xlsx/csv)",
                type=["xlsx", "csv"],
                key="admin_upload_file"
            )

            if st.button("Ch·∫°y ki·ªÉm tra file Admin"):
                if file_admin is None:
                    st.error("Vui l√≤ng upload file tr∆∞·ªõc!")
                else:
                    with tempfile.NamedTemporaryFile(
                        delete=False,
                        suffix=os.path.splitext(file_admin.name)[1]
                    ) as tmp:
                        tmp.write(file_admin.getvalue())
                        excel_path = tmp.name

                    try:
                        df_in = preprocess_motobike_data(excel_path)
                        df_all, anomaly = detect_outliers(
                            df_in, 
                            model_path=MODEL_PATH, 
                            input_is_df=True, 
                            helpers=helpers
                        )

                        st.success(
                            f"Ho√†n t·∫•t ki·ªÉm tra. T·ªïng {len(df_in)} b·∫£n ghi ‚Äî ph√°t hi·ªán {len(anomaly)} b·∫•t th∆∞·ªùng."
                        )
                        # st.dataframe(anomaly.head(20))
                        anomaly_print = anomaly.copy()
                        cols_to_drop = ['brand_grouped', 'model_grouped', 'segment', 'brand_meanprice','price_hat','resid','resid_median','resid_std','resid_z','flag_resid','p10','p90'
]
                        anomaly_print = anomaly_print.drop(columns=[c for c in cols_to_drop if c in anomaly_print.columns])
                        st.dataframe(anomaly_print.head(20))

                        # === B·∫ÆT ƒê·∫¶U TH√äM N√öT T·∫¢I XU·ªêNG ===
                        if len(anomaly) > 0:
                            # 1. T·∫°o t√™n file c√≥ ng√†y gi·ªù
                            now = datetime.now().strftime("%Y%m%d_%H%M%S")
                            file_name = f"anomaly_detection_admin_{now}.csv"
                            
                            # 2. Chuy·ªÉn DataFrame sang CSV
                            df_output = anomaly_print.copy()
                            # N·∫øu c·ªôt post_time c√≥, h√£y lo·∫°i b·ªè m√∫i gi·ªù (ƒë·ªÉ tr√°nh l·ªói)
                            if 'post_time' in df_output.columns and df_output['post_time'].dt.tz is not None:
                                df_output['post_time'] = df_output['post_time'].dt.tz_localize(None)

                            csv = df_output.to_csv(index=False).encode('utf-8')
                            
                            # 3. T·∫°o n√∫t t·∫£i xu·ªëng
                            st.download_button(
                                label="T·∫£i k·∫øt qu·∫£ b·∫•t th∆∞·ªùng (CSV)",
                                data=csv,
                                file_name=file_name,
                                mime='text/csv'
                            )
                        # === K·∫æT TH√öC TH√äM N√öT T·∫¢I XU·ªêNG ===

                    except Exception as e:
                        st.exception(e)

    # ======================================
    # 2) TAB DASHBOARD
    # ======================================
    with tab_dash:
        st.title("üìä Dashboard Qu·∫£n L√Ω ‚Äì Motorbike Marketplace")
        st.markdown("Theo d√µi tr·∫°ng th√°i tin ng∆∞·ªùi d√πng g·ª≠i, hi·ªáu qu·∫£ duy·ªát v√† ph√¢n kh√∫c tin ƒëƒÉng real-time.")

        # =====================================
        # LOAD DATA
        # =====================================
        df_user = pd.read_excel("user_submissions.xlsx")
        df_real = pd.read_excel("data_motobikes_realtime.xlsx")

        df_user["post_time"] = pd.to_datetime(df_user["post_time"], errors="coerce")

        # Pending & approved
        df_pending = df_user[df_user["approved"] == False]
        df_approved = df_real

        # =====================================
        # KPI CARDS
        # =====================================
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("üìù T·ªïng tin user g·ª≠i trong ng√†y", len(df_user))
        c2.metric("‚è≥ Tin ch∆∞a duy·ªát", len(df_pending))
        c3.metric("‚úÖ Tin ƒë√£ duy·ªát", len(df_approved))
        c4.metric("üåê T·ªïng tin h·ªá th·ªëng", len(df_user) + len(df_approved))
        rate = round(len(df_approved) * 100 / (len(df_user) + len(df_approved)), 1) if (len(df_user) + len(df_approved)) > 0 else 0
        c5.metric("üìà T·ª∑ l·ªá duy·ªát (%)", f"{rate} %")

        st.markdown("---")

        # =====================================
        # CHART 1: Pie chart status
        # =====================================
        st.subheader("üîç Tr·∫°ng th√°i duy·ªát tin")
        fig1 = go.Figure(data=[go.Pie(
            labels=["Pending", "Approved"],
            values=[len(df_pending), len(df_approved)],
            hole=.45
        )])
        fig1.update_layout(height=300, template="plotly_white")
        st.plotly_chart(fig1, use_container_width=True)

        # =====================================
        # CHART 2: Top 10 khu v·ª±c
        # =====================================
        if "ƒê·ªãa ch·ªâ" in df_real.columns:
            st.subheader("üìç Top 10 khu v·ª±c c√≥ nhi·ªÅu tin ƒëƒÉng nh·∫•t")
            df_area = df_real["ƒê·ªãa ch·ªâ"].value_counts().head(10).reset_index()
            df_area.columns = ["ƒê·ªãa ch·ªâ", "count"]
            fig2 = go.Figure([go.Bar(
                x=df_area["count"],
                y=df_area["ƒê·ªãa ch·ªâ"],
                orientation="h",
                marker_color="skyblue"
            )])
            fig2.update_layout(height=350, template="plotly_white", yaxis=dict(autorange="reversed"))
            st.plotly_chart(fig2, use_container_width=True)

        # =====================================
        # CHART 3: Cluster distribution (Realtime)
        # =====================================
        if "cluster_label" in df_real.columns:
            st.subheader("üéØ Ph√¢n b·ªë Ph√¢n kh√∫c xe")
            cluster_count = df_real["cluster_label"].value_counts().reset_index()
            cluster_count.columns = ["cluster_label", "count"]
            fig3 = go.Figure([go.Bar(
                x=cluster_count["cluster_label"].astype(str),
                y=cluster_count["count"],
                marker_color="mediumpurple"
            )])
            fig3.update_layout(height=350, template="plotly_white", xaxis_title="Cluster", yaxis_title="S·ªë tin")
            st.plotly_chart(fig3, use_container_width=True)

        st.markdown("---")

        # =====================================
        # TABLE: Pending items
        # =====================================
        st.subheader("üìå Danh s√°ch tin ch∆∞a duy·ªát")
        display_cols = ["title", "description","brand","model","price", "address", "post_time"]
        st.dataframe(df_pending[display_cols], use_container_width=True, height=350)


